{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 [신경망 복습]\n",
    "---\n",
    "* 이번 장에서는 밑바닥부터 시작하는 딥러닝1(전편) 에서 구현한 신경망을 복습합니다.<br>\n",
    "* 이 책에서는 효율을 높이고자 전편에서 구현한 것을 일부 변경했습니다.<br>\n",
    "* 마지막에는 신경망 고속화를 위해 GPU 라이브러리(쿠파이)를 사용합니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.신경망\n",
    "신경망은 단순한 '함수'라고 할 수 있습니다.<br>\n",
    "함수란 어떠한 입력을 받으면 어떠한 출력을 리턴합니다.<br>\n",
    "신경망도 함수처럼 입력을 받으면 어떠한 출력을 리턴합니다.<br>\n",
    "\n",
    "<img src=img/fig1-7.png width='300' align='left'> <img src=img/dumy.png width='250'>\n",
    "<br>\n",
    "그림 1-7은 신경망을 표현한 그림입니다.<br>\n",
    "입력층의 뉴런으로 데이터가 입력되면 화살표를 따라 다음층의 뉴런으로 흘러갑니다.<br>\n",
    "이때 각 화살표에는 <b>가중치</b><sup>weight</sup>가 존재하며, 뉴런의 값과 가중치들을 곱해서 그 합이 다음 뉴런의 입력으로 쓰입니다.<br>\n",
    "또, 이때 각 층에서는 이전 뉴런의 값에 영향받지 않는 '정수'도 더해지는데, 이 정수는 <b>편향</b><sup>bias</sup>이라고 합니다.<br>\n",
    "이렇게 그림 1-7의 신경망은 인접하는 층의 모든 뉴런과 연결되어 있다는 뜻에서 <b>완전연결계층</b><sup>fully connected layer</sup>이라고 합니다.<br>\n",
    "\n",
    "그림 1-7의 은닉층 중 첫 번째 뉴런은 다음과 같이 계산할 수 있습니다.<br>\n",
    "$$ h_{1} = x_{1}w_{11} + x_{2}w_{21} + b_{1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.활성화 함수\n",
    "완전연결계층에 의한 변환은 '선형'변환입니다.<br>\n",
    "그러나 비선형 활성화 함수를 사용함으로써 신경망의 표현력을 높일 수 있습니다.<br>\n",
    "이번 예제에서는 <b>시그모이드 함수</b><sup>sigmoid function</sup>를 사용합니다.<br>\n",
    "$$ \\sigma(x)=\\frac{1}{1+exp(-x)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.신경망 학습\n",
    "\n",
    "### 3-1 손실 함수\n",
    "신경망 학습에느 학습이 얼마나 잘 되고 있는지를 알기 위한 '척도'가 필요합니다.<br>\n",
    "신경망 학습 시 성능을 나타내는 척도로 <b>손실</b><sup>loss</sup>을 사용합니다.<br>\n",
    "신경망의 손실은 <b>손실 함수</b><sup>loss function</sup>를 사용해 구합니다.<br>\n",
    "다중 클래스 분류 신경망에서는 손실 함수로 흔히 <b>교차 엔트로피 오차</b><sup>Cross Entropy Error</sup>를 이용합니다.<br>\n",
    "<b>교차 엔트로피 오차</b><sup>Cross Entropy Error</sup>의 수식은 다음과 같습니다.<br>\n",
    "$$ L=-\\sum_{k}^{}t_{k}logy_{k} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 미분과 기울기\n",
    "신경망 학습의 목표는 손실을 최소화하는 매개변수를 찾는 것입니다. 이때 중요한 것이 '미분'과 '기울기'입니다.<br>\n",
    "\n",
    "어떤 함수 $y=f(x)$가 있다고 합시다. 이때 $x$에 관한 $y$의 미분은 $\\frac{dy}{dx}$라고 씁니다.<br>\n",
    "이 $\\frac{dy}{dx}$가 의미하는 것은 $x$의 값을 '아주조금'(극한까지 줄일 때)변화시켰을 때 $y$의 값이 얼마나 변하는가 하는 '변화의 정도'입니다.<br>\n",
    "\n",
    "$y = x^{2}$이라는 함수를 예로 살펴보겠습니다.<br>\n",
    "이 함수의 미분을 해석적으로 구하면 $\\frac{dy}{dx} = 2x$가 됩니다.<br>\n",
    "이 미분 결과는 각 $x$에서의 변화의 정도를 뜻하며, 그림1-14에서 보듯 함수의 '기울기'에 해당합니다.<br>\n",
    "<img src=img/fig1-14.png width='300' align='left'> <img src=img/dumy.png width='250'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3 연쇄 법칙\n",
    "신경망의 기울기를 구하는데 <b>오차역전파법</b><sup>back-propagation</sup>을 사용합니다.<br>\n",
    "오차역전파법을 이해하기 위해서 <b>연쇄 법칙</b><sup>chain rule</sup>을 알아보겠습니다.<br>\n",
    "연쇄 법칙이란 합성함수에 대한 미분의 법칙입니다.<br>\n",
    "합성함수란 여러 함수로 구성된 함수이고, 신경망은 여러 활성화 함수를 통과하며 손실값을 구하게됩니다.<br>\n",
    "\n",
    "연쇄 법칙을 자세히 알아보겠습니다. 예를 들어, $y = f(x)$와 $z = g(y)$라는 두 함수가 있습니다.<br>\n",
    "그러면 $z = g(f(x))$가 되어, 최종 출력 $z$는 두 함수를 조합해 계산할 수 있습니다.<br>\n",
    "이때 합성함수의 미분($x$에 대한 $z$의 미분)은 다음과 같이 구할 수 있습니다.<br>\n",
    "$$\\frac{\\partial z}{\\partial x}=\\frac{\\partial z}{\\partial \\not y}\\frac{\\partial \\not y}{\\partial x}$$\n",
    "위 식처럼, $x$에 대한 $z$의 미분은 $y = f(x)$의 미분과 $z = g(y)$의 미분을 곱하면 구할 수 있습니다.<br>\n",
    "\n",
    "이 연쇄 법칙이 중요한 이유는 아무리 많은 함수를 연결하더라도, 그 미분은 개별 함수의 미분을 이용해 구할 수 있기 때문입니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.신경망 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "\n",
    "        W1 = 0.01 * np.random.randn(I, H)\n",
    "        b1 = np.zeros(H)\n",
    "        W2 = 0.01 * np.random.randn(H, O)\n",
    "        b2 = np.zeros(O)\n",
    "\n",
    "        self.layers = [Affine(W1, b1), Sigmoid(), Affine(W2, b2)]\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        score = self.predict(x)\n",
    "        loss = self.loss_layer.forward(score, t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = softmax(x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = self.out * dout\n",
    "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
    "        dx -= self.out * sumdx\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "    \n",
    "    \n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]\n",
    "            \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 정답 데이터가 원핫 벡터일 경우 정답 레이블 인덱스로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 10 / 10 | 손실 1.13\n",
      "| 에폭 2 |  반복 10 / 10 | 손실 1.13\n",
      "| 에폭 3 |  반복 10 / 10 | 손실 1.12\n",
      "| 에폭 4 |  반복 10 / 10 | 손실 1.12\n",
      "| 에폭 5 |  반복 10 / 10 | 손실 1.11\n",
      "| 에폭 6 |  반복 10 / 10 | 손실 1.14\n",
      "| 에폭 7 |  반복 10 / 10 | 손실 1.16\n",
      "| 에폭 8 |  반복 10 / 10 | 손실 1.11\n",
      "| 에폭 9 |  반복 10 / 10 | 손실 1.12\n",
      "| 에폭 10 |  반복 10 / 10 | 손실 1.13\n",
      "| 에폭 11 |  반복 10 / 10 | 손실 1.12\n",
      "| 에폭 12 |  반복 10 / 10 | 손실 1.11\n",
      "| 에폭 13 |  반복 10 / 10 | 손실 1.09\n",
      "| 에폭 14 |  반복 10 / 10 | 손실 1.08\n",
      "| 에폭 15 |  반복 10 / 10 | 손실 1.04\n",
      "| 에폭 16 |  반복 10 / 10 | 손실 1.03\n",
      "| 에폭 17 |  반복 10 / 10 | 손실 0.96\n",
      "| 에폭 18 |  반복 10 / 10 | 손실 0.92\n",
      "| 에폭 19 |  반복 10 / 10 | 손실 0.92\n",
      "| 에폭 20 |  반복 10 / 10 | 손실 0.87\n",
      "| 에폭 21 |  반복 10 / 10 | 손실 0.85\n",
      "| 에폭 22 |  반복 10 / 10 | 손실 0.82\n",
      "| 에폭 23 |  반복 10 / 10 | 손실 0.79\n",
      "| 에폭 24 |  반복 10 / 10 | 손실 0.78\n",
      "| 에폭 25 |  반복 10 / 10 | 손실 0.82\n",
      "| 에폭 26 |  반복 10 / 10 | 손실 0.78\n",
      "| 에폭 27 |  반복 10 / 10 | 손실 0.76\n",
      "| 에폭 28 |  반복 10 / 10 | 손실 0.76\n",
      "| 에폭 29 |  반복 10 / 10 | 손실 0.78\n",
      "| 에폭 30 |  반복 10 / 10 | 손실 0.75\n",
      "| 에폭 31 |  반복 10 / 10 | 손실 0.78\n",
      "| 에폭 32 |  반복 10 / 10 | 손실 0.77\n",
      "| 에폭 33 |  반복 10 / 10 | 손실 0.77\n",
      "| 에폭 34 |  반복 10 / 10 | 손실 0.78\n",
      "| 에폭 35 |  반복 10 / 10 | 손실 0.75\n",
      "| 에폭 36 |  반복 10 / 10 | 손실 0.74\n",
      "| 에폭 37 |  반복 10 / 10 | 손실 0.76\n",
      "| 에폭 38 |  반복 10 / 10 | 손실 0.76\n",
      "| 에폭 39 |  반복 10 / 10 | 손실 0.73\n",
      "| 에폭 40 |  반복 10 / 10 | 손실 0.75\n",
      "| 에폭 41 |  반복 10 / 10 | 손실 0.76\n",
      "| 에폭 42 |  반복 10 / 10 | 손실 0.76\n",
      "| 에폭 43 |  반복 10 / 10 | 손실 0.76\n",
      "| 에폭 44 |  반복 10 / 10 | 손실 0.74\n",
      "| 에폭 45 |  반복 10 / 10 | 손실 0.75\n",
      "| 에폭 46 |  반복 10 / 10 | 손실 0.73\n",
      "| 에폭 47 |  반복 10 / 10 | 손실 0.72\n",
      "| 에폭 48 |  반복 10 / 10 | 손실 0.73\n",
      "| 에폭 49 |  반복 10 / 10 | 손실 0.72\n",
      "| 에폭 50 |  반복 10 / 10 | 손실 0.72\n",
      "| 에폭 51 |  반복 10 / 10 | 손실 0.72\n",
      "| 에폭 52 |  반복 10 / 10 | 손실 0.72\n",
      "| 에폭 53 |  반복 10 / 10 | 손실 0.74\n",
      "| 에폭 54 |  반복 10 / 10 | 손실 0.74\n",
      "| 에폭 55 |  반복 10 / 10 | 손실 0.72\n",
      "| 에폭 56 |  반복 10 / 10 | 손실 0.72\n",
      "| 에폭 57 |  반복 10 / 10 | 손실 0.71\n",
      "| 에폭 58 |  반복 10 / 10 | 손실 0.70\n",
      "| 에폭 59 |  반복 10 / 10 | 손실 0.72\n",
      "| 에폭 60 |  반복 10 / 10 | 손실 0.70\n",
      "| 에폭 61 |  반복 10 / 10 | 손실 0.71\n",
      "| 에폭 62 |  반복 10 / 10 | 손실 0.72\n",
      "| 에폭 63 |  반복 10 / 10 | 손실 0.70\n",
      "| 에폭 64 |  반복 10 / 10 | 손실 0.71\n",
      "| 에폭 65 |  반복 10 / 10 | 손실 0.73\n",
      "| 에폭 66 |  반복 10 / 10 | 손실 0.70\n",
      "| 에폭 67 |  반복 10 / 10 | 손실 0.71\n",
      "| 에폭 68 |  반복 10 / 10 | 손실 0.69\n",
      "| 에폭 69 |  반복 10 / 10 | 손실 0.70\n",
      "| 에폭 70 |  반복 10 / 10 | 손실 0.71\n",
      "| 에폭 71 |  반복 10 / 10 | 손실 0.68\n",
      "| 에폭 72 |  반복 10 / 10 | 손실 0.69\n",
      "| 에폭 73 |  반복 10 / 10 | 손실 0.67\n",
      "| 에폭 74 |  반복 10 / 10 | 손실 0.68\n",
      "| 에폭 75 |  반복 10 / 10 | 손실 0.67\n",
      "| 에폭 76 |  반복 10 / 10 | 손실 0.66\n",
      "| 에폭 77 |  반복 10 / 10 | 손실 0.69\n",
      "| 에폭 78 |  반복 10 / 10 | 손실 0.64\n",
      "| 에폭 79 |  반복 10 / 10 | 손실 0.68\n",
      "| 에폭 80 |  반복 10 / 10 | 손실 0.64\n",
      "| 에폭 81 |  반복 10 / 10 | 손실 0.64\n",
      "| 에폭 82 |  반복 10 / 10 | 손실 0.66\n",
      "| 에폭 83 |  반복 10 / 10 | 손실 0.62\n",
      "| 에폭 84 |  반복 10 / 10 | 손실 0.62\n",
      "| 에폭 85 |  반복 10 / 10 | 손실 0.61\n",
      "| 에폭 86 |  반복 10 / 10 | 손실 0.60\n",
      "| 에폭 87 |  반복 10 / 10 | 손실 0.60\n",
      "| 에폭 88 |  반복 10 / 10 | 손실 0.61\n",
      "| 에폭 89 |  반복 10 / 10 | 손실 0.59\n",
      "| 에폭 90 |  반복 10 / 10 | 손실 0.58\n",
      "| 에폭 91 |  반복 10 / 10 | 손실 0.56\n",
      "| 에폭 92 |  반복 10 / 10 | 손실 0.56\n",
      "| 에폭 93 |  반복 10 / 10 | 손실 0.54\n",
      "| 에폭 94 |  반복 10 / 10 | 손실 0.53\n",
      "| 에폭 95 |  반복 10 / 10 | 손실 0.53\n",
      "| 에폭 96 |  반복 10 / 10 | 손실 0.52\n",
      "| 에폭 97 |  반복 10 / 10 | 손실 0.51\n",
      "| 에폭 98 |  반복 10 / 10 | 손실 0.50\n",
      "| 에폭 99 |  반복 10 / 10 | 손실 0.48\n",
      "| 에폭 100 |  반복 10 / 10 | 손실 0.48\n",
      "| 에폭 101 |  반복 10 / 10 | 손실 0.46\n",
      "| 에폭 102 |  반복 10 / 10 | 손실 0.45\n",
      "| 에폭 103 |  반복 10 / 10 | 손실 0.45\n",
      "| 에폭 104 |  반복 10 / 10 | 손실 0.44\n",
      "| 에폭 105 |  반복 10 / 10 | 손실 0.44\n",
      "| 에폭 106 |  반복 10 / 10 | 손실 0.41\n",
      "| 에폭 107 |  반복 10 / 10 | 손실 0.40\n",
      "| 에폭 108 |  반복 10 / 10 | 손실 0.41\n",
      "| 에폭 109 |  반복 10 / 10 | 손실 0.40\n",
      "| 에폭 110 |  반복 10 / 10 | 손실 0.40\n",
      "| 에폭 111 |  반복 10 / 10 | 손실 0.38\n",
      "| 에폭 112 |  반복 10 / 10 | 손실 0.38\n",
      "| 에폭 113 |  반복 10 / 10 | 손실 0.36\n",
      "| 에폭 114 |  반복 10 / 10 | 손실 0.37\n",
      "| 에폭 115 |  반복 10 / 10 | 손실 0.35\n",
      "| 에폭 116 |  반복 10 / 10 | 손실 0.34\n",
      "| 에폭 117 |  반복 10 / 10 | 손실 0.34\n",
      "| 에폭 118 |  반복 10 / 10 | 손실 0.34\n",
      "| 에폭 119 |  반복 10 / 10 | 손실 0.33\n",
      "| 에폭 120 |  반복 10 / 10 | 손실 0.34\n",
      "| 에폭 121 |  반복 10 / 10 | 손실 0.32\n",
      "| 에폭 122 |  반복 10 / 10 | 손실 0.32\n",
      "| 에폭 123 |  반복 10 / 10 | 손실 0.31\n",
      "| 에폭 124 |  반복 10 / 10 | 손실 0.31\n",
      "| 에폭 125 |  반복 10 / 10 | 손실 0.30\n",
      "| 에폭 126 |  반복 10 / 10 | 손실 0.30\n",
      "| 에폭 127 |  반복 10 / 10 | 손실 0.28\n",
      "| 에폭 128 |  반복 10 / 10 | 손실 0.28\n",
      "| 에폭 129 |  반복 10 / 10 | 손실 0.28\n",
      "| 에폭 130 |  반복 10 / 10 | 손실 0.28\n",
      "| 에폭 131 |  반복 10 / 10 | 손실 0.27\n",
      "| 에폭 132 |  반복 10 / 10 | 손실 0.27\n",
      "| 에폭 133 |  반복 10 / 10 | 손실 0.27\n",
      "| 에폭 134 |  반복 10 / 10 | 손실 0.27\n",
      "| 에폭 135 |  반복 10 / 10 | 손실 0.27\n",
      "| 에폭 136 |  반복 10 / 10 | 손실 0.26\n",
      "| 에폭 137 |  반복 10 / 10 | 손실 0.26\n",
      "| 에폭 138 |  반복 10 / 10 | 손실 0.26\n",
      "| 에폭 139 |  반복 10 / 10 | 손실 0.25\n",
      "| 에폭 140 |  반복 10 / 10 | 손실 0.24\n",
      "| 에폭 141 |  반복 10 / 10 | 손실 0.24\n",
      "| 에폭 142 |  반복 10 / 10 | 손실 0.25\n",
      "| 에폭 143 |  반복 10 / 10 | 손실 0.24\n",
      "| 에폭 144 |  반복 10 / 10 | 손실 0.24\n",
      "| 에폭 145 |  반복 10 / 10 | 손실 0.23\n",
      "| 에폭 146 |  반복 10 / 10 | 손실 0.24\n",
      "| 에폭 147 |  반복 10 / 10 | 손실 0.23\n",
      "| 에폭 148 |  반복 10 / 10 | 손실 0.23\n",
      "| 에폭 149 |  반복 10 / 10 | 손실 0.22\n",
      "| 에폭 150 |  반복 10 / 10 | 손실 0.22\n",
      "| 에폭 151 |  반복 10 / 10 | 손실 0.22\n",
      "| 에폭 152 |  반복 10 / 10 | 손실 0.22\n",
      "| 에폭 153 |  반복 10 / 10 | 손실 0.22\n",
      "| 에폭 154 |  반복 10 / 10 | 손실 0.22\n",
      "| 에폭 155 |  반복 10 / 10 | 손실 0.22\n",
      "| 에폭 156 |  반복 10 / 10 | 손실 0.21\n",
      "| 에폭 157 |  반복 10 / 10 | 손실 0.21\n",
      "| 에폭 158 |  반복 10 / 10 | 손실 0.20\n",
      "| 에폭 159 |  반복 10 / 10 | 손실 0.21\n",
      "| 에폭 160 |  반복 10 / 10 | 손실 0.20\n",
      "| 에폭 161 |  반복 10 / 10 | 손실 0.20\n",
      "| 에폭 162 |  반복 10 / 10 | 손실 0.20\n",
      "| 에폭 163 |  반복 10 / 10 | 손실 0.21\n",
      "| 에폭 164 |  반복 10 / 10 | 손실 0.20\n",
      "| 에폭 165 |  반복 10 / 10 | 손실 0.20\n",
      "| 에폭 166 |  반복 10 / 10 | 손실 0.19\n",
      "| 에폭 167 |  반복 10 / 10 | 손실 0.19\n",
      "| 에폭 168 |  반복 10 / 10 | 손실 0.19\n",
      "| 에폭 169 |  반복 10 / 10 | 손실 0.19\n",
      "| 에폭 170 |  반복 10 / 10 | 손실 0.19\n",
      "| 에폭 171 |  반복 10 / 10 | 손실 0.19\n",
      "| 에폭 172 |  반복 10 / 10 | 손실 0.18\n",
      "| 에폭 173 |  반복 10 / 10 | 손실 0.18\n",
      "| 에폭 174 |  반복 10 / 10 | 손실 0.18\n",
      "| 에폭 175 |  반복 10 / 10 | 손실 0.18\n",
      "| 에폭 176 |  반복 10 / 10 | 손실 0.18\n",
      "| 에폭 177 |  반복 10 / 10 | 손실 0.18\n",
      "| 에폭 178 |  반복 10 / 10 | 손실 0.18\n",
      "| 에폭 179 |  반복 10 / 10 | 손실 0.17\n",
      "| 에폭 180 |  반복 10 / 10 | 손실 0.17\n",
      "| 에폭 181 |  반복 10 / 10 | 손실 0.18\n",
      "| 에폭 182 |  반복 10 / 10 | 손실 0.17\n",
      "| 에폭 183 |  반복 10 / 10 | 손실 0.18\n",
      "| 에폭 184 |  반복 10 / 10 | 손실 0.17\n",
      "| 에폭 185 |  반복 10 / 10 | 손실 0.17\n",
      "| 에폭 186 |  반복 10 / 10 | 손실 0.18\n",
      "| 에폭 187 |  반복 10 / 10 | 손실 0.17\n",
      "| 에폭 188 |  반복 10 / 10 | 손실 0.17\n",
      "| 에폭 189 |  반복 10 / 10 | 손실 0.17\n",
      "| 에폭 190 |  반복 10 / 10 | 손실 0.17\n",
      "| 에폭 191 |  반복 10 / 10 | 손실 0.16\n",
      "| 에폭 192 |  반복 10 / 10 | 손실 0.17\n",
      "| 에폭 193 |  반복 10 / 10 | 손실 0.16\n",
      "| 에폭 194 |  반복 10 / 10 | 손실 0.16\n",
      "| 에폭 195 |  반복 10 / 10 | 손실 0.16\n",
      "| 에폭 196 |  반복 10 / 10 | 손실 0.16\n",
      "| 에폭 197 |  반복 10 / 10 | 손실 0.16\n",
      "| 에폭 198 |  반복 10 / 10 | 손실 0.15\n",
      "| 에폭 199 |  반복 10 / 10 | 손실 0.16\n",
      "| 에폭 200 |  반복 10 / 10 | 손실 0.16\n",
      "| 에폭 201 |  반복 10 / 10 | 손실 0.15\n",
      "| 에폭 202 |  반복 10 / 10 | 손실 0.16\n",
      "| 에폭 203 |  반복 10 / 10 | 손실 0.16\n",
      "| 에폭 204 |  반복 10 / 10 | 손실 0.15\n",
      "| 에폭 205 |  반복 10 / 10 | 손실 0.16\n",
      "| 에폭 206 |  반복 10 / 10 | 손실 0.15\n",
      "| 에폭 207 |  반복 10 / 10 | 손실 0.15\n",
      "| 에폭 208 |  반복 10 / 10 | 손실 0.15\n",
      "| 에폭 209 |  반복 10 / 10 | 손실 0.15\n",
      "| 에폭 210 |  반복 10 / 10 | 손실 0.15\n",
      "| 에폭 211 |  반복 10 / 10 | 손실 0.15\n",
      "| 에폭 212 |  반복 10 / 10 | 손실 0.15\n",
      "| 에폭 213 |  반복 10 / 10 | 손실 0.15\n",
      "| 에폭 214 |  반복 10 / 10 | 손실 0.15\n",
      "| 에폭 215 |  반복 10 / 10 | 손실 0.15\n",
      "| 에폭 216 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 217 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 218 |  반복 10 / 10 | 손실 0.15\n",
      "| 에폭 219 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 220 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 221 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 222 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 223 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 224 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 225 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 226 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 227 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 228 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 229 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 230 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 231 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 232 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 233 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 234 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 235 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 236 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 237 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 238 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 239 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 240 |  반복 10 / 10 | 손실 0.14\n",
      "| 에폭 241 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 242 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 243 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 244 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 245 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 246 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 247 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 248 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 249 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 250 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 251 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 252 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 253 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 254 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 255 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 256 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 257 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 258 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 259 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 260 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 261 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 262 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 263 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 264 |  반복 10 / 10 | 손실 0.13\n",
      "| 에폭 265 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 266 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 267 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 268 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 269 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 270 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 271 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 272 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 273 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 274 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 275 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 276 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 277 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 278 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 279 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 280 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 281 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 282 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 283 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 284 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 285 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 286 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 287 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 288 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 289 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 290 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 291 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 292 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 293 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 294 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 295 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 296 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 297 |  반복 10 / 10 | 손실 0.12\n",
      "| 에폭 298 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 299 |  반복 10 / 10 | 손실 0.11\n",
      "| 에폭 300 |  반복 10 / 10 | 손실 0.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dud19\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\dud19\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\dud19\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\dud19\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\dud19\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 49552 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\dud19\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:211: RuntimeWarning: Glyph 49892 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\dud19\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 49552 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\dud19\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:180: RuntimeWarning: Glyph 49892 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습시간 : 0.53초\n"
     ]
    }
   ],
   "source": [
    "from dataset import spiral\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "\n",
    "# 데이터 읽기, 모델과 옵티마이저 생성\n",
    "x, t = spiral.load_data()\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "# 학습에 사용하는 변수\n",
    "data_size = len(x)\n",
    "max_iters = data_size // batch_size\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "loss_list = []\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(max_epoch):\n",
    "    # 데이터 뒤섞기\n",
    "    idx = np.random.permutation(data_size)\n",
    "    x = x[idx]\n",
    "    t = t[idx]\n",
    "\n",
    "    for iters in range(max_iters):\n",
    "        batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "        batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "        # 기울기를 구해 매개변수 갱신\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "        # 정기적으로 학습 경과 출력\n",
    "        if (iters+1) % 10 == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            print('| 에폭 %d |  반복 %d / %d | 손실 %.2f'\n",
    "                  % (epoch + 1, iters + 1, max_iters, avg_loss))\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0, 0\n",
    "end = time.time()\n",
    "\n",
    "# 학습 결과 플롯\n",
    "plt.plot(np.arange(len(loss_list)), loss_list, label='train')\n",
    "plt.xlabel('반복 (x10)')\n",
    "plt.ylabel('손실')\n",
    "plt.show()\n",
    "\n",
    "# 경계 영역 플롯\n",
    "h = 0.001\n",
    "x_min, x_max = x[:, 0].min() - .1, x[:, 0].max() + .1\n",
    "y_min, y_max = x[:, 1].min() - .1, x[:, 1].max() + .1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "X = np.c_[xx.ravel(), yy.ravel()]\n",
    "score = model.predict(X)\n",
    "predict_cls = np.argmax(score, axis=1)\n",
    "Z = predict_cls.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z)\n",
    "plt.axis('off')\n",
    "\n",
    "# 데이터점 플롯\n",
    "x, t = spiral.load_data()\n",
    "N = 100\n",
    "CLS_NUM = 3\n",
    "markers = ['o', 'x', '^']\n",
    "for i in range(CLS_NUM):\n",
    "    plt.scatter(x[i*N:(i+1)*N, 0], x[i*N:(i+1)*N, 1], s=40, marker=markers[i])\n",
    "plt.show()\n",
    "print(\"학습시간 : {0:.2f}초\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.계산 고속화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST(CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 10 / 60 | 손실 2.30\n",
      "| 에폭 1 |  반복 20 / 60 | 손실 2.30\n",
      "| 에폭 1 |  반복 30 / 60 | 손실 2.30\n",
      "| 에폭 1 |  반복 40 / 60 | 손실 2.29\n",
      "| 에폭 1 |  반복 50 / 60 | 손실 2.29\n",
      "| 에폭 1 |  반복 60 / 60 | 손실 2.29\n",
      "| 에폭 2 |  반복 10 / 60 | 손실 2.28\n",
      "| 에폭 2 |  반복 20 / 60 | 손실 2.28\n",
      "| 에폭 2 |  반복 30 / 60 | 손실 2.27\n",
      "| 에폭 2 |  반복 40 / 60 | 손실 2.27\n",
      "| 에폭 2 |  반복 50 / 60 | 손실 2.26\n",
      "| 에폭 2 |  반복 60 / 60 | 손실 2.24\n",
      "| 에폭 3 |  반복 10 / 60 | 손실 2.23\n",
      "| 에폭 3 |  반복 20 / 60 | 손실 2.21\n",
      "| 에폭 3 |  반복 30 / 60 | 손실 2.18\n",
      "| 에폭 3 |  반복 40 / 60 | 손실 2.16\n",
      "| 에폭 3 |  반복 50 / 60 | 손실 2.13\n",
      "| 에폭 3 |  반복 60 / 60 | 손실 2.10\n",
      "| 에폭 4 |  반복 10 / 60 | 손실 2.05\n",
      "| 에폭 4 |  반복 20 / 60 | 손실 2.01\n",
      "| 에폭 4 |  반복 30 / 60 | 손실 1.96\n",
      "| 에폭 4 |  반복 40 / 60 | 손실 1.90\n",
      "| 에폭 4 |  반복 50 / 60 | 손실 1.85\n",
      "| 에폭 4 |  반복 60 / 60 | 손실 1.79\n",
      "| 에폭 5 |  반복 10 / 60 | 손실 1.74\n",
      "| 에폭 5 |  반복 20 / 60 | 손실 1.67\n",
      "| 에폭 5 |  반복 30 / 60 | 손실 1.62\n",
      "| 에폭 5 |  반복 40 / 60 | 손실 1.57\n",
      "| 에폭 5 |  반복 50 / 60 | 손실 1.51\n",
      "| 에폭 5 |  반복 60 / 60 | 손실 1.46\n",
      "| 에폭 6 |  반복 10 / 60 | 손실 1.41\n",
      "| 에폭 6 |  반복 20 / 60 | 손실 1.36\n",
      "| 에폭 6 |  반복 30 / 60 | 손실 1.32\n",
      "| 에폭 6 |  반복 40 / 60 | 손실 1.27\n",
      "| 에폭 6 |  반복 50 / 60 | 손실 1.24\n",
      "| 에폭 6 |  반복 60 / 60 | 손실 1.19\n",
      "| 에폭 7 |  반복 10 / 60 | 손실 1.15\n",
      "| 에폭 7 |  반복 20 / 60 | 손실 1.13\n",
      "| 에폭 7 |  반복 30 / 60 | 손실 1.10\n",
      "| 에폭 7 |  반복 40 / 60 | 손실 1.07\n",
      "| 에폭 7 |  반복 50 / 60 | 손실 1.03\n",
      "| 에폭 7 |  반복 60 / 60 | 손실 1.01\n",
      "| 에폭 8 |  반복 10 / 60 | 손실 0.98\n",
      "| 에폭 8 |  반복 20 / 60 | 손실 0.96\n",
      "| 에폭 8 |  반복 30 / 60 | 손실 0.93\n",
      "| 에폭 8 |  반복 40 / 60 | 손실 0.92\n",
      "| 에폭 8 |  반복 50 / 60 | 손실 0.89\n",
      "| 에폭 8 |  반복 60 / 60 | 손실 0.88\n",
      "| 에폭 9 |  반복 10 / 60 | 손실 0.85\n",
      "| 에폭 9 |  반복 20 / 60 | 손실 0.84\n",
      "| 에폭 9 |  반복 30 / 60 | 손실 0.83\n",
      "| 에폭 9 |  반복 40 / 60 | 손실 0.80\n",
      "| 에폭 9 |  반복 50 / 60 | 손실 0.80\n",
      "| 에폭 9 |  반복 60 / 60 | 손실 0.76\n",
      "| 에폭 10 |  반복 10 / 60 | 손실 0.76\n",
      "| 에폭 10 |  반복 20 / 60 | 손실 0.75\n",
      "| 에폭 10 |  반복 30 / 60 | 손실 0.75\n",
      "| 에폭 10 |  반복 40 / 60 | 손실 0.72\n",
      "| 에폭 10 |  반복 50 / 60 | 손실 0.71\n",
      "| 에폭 10 |  반복 60 / 60 | 손실 0.70\n",
      "| 에폭 11 |  반복 10 / 60 | 손실 0.69\n",
      "| 에폭 11 |  반복 20 / 60 | 손실 0.68\n",
      "| 에폭 11 |  반복 30 / 60 | 손실 0.68\n",
      "| 에폭 11 |  반복 40 / 60 | 손실 0.67\n",
      "| 에폭 11 |  반복 50 / 60 | 손실 0.65\n",
      "| 에폭 11 |  반복 60 / 60 | 손실 0.64\n",
      "| 에폭 12 |  반복 10 / 60 | 손실 0.64\n",
      "| 에폭 12 |  반복 20 / 60 | 손실 0.63\n",
      "| 에폭 12 |  반복 30 / 60 | 손실 0.62\n",
      "| 에폭 12 |  반복 40 / 60 | 손실 0.62\n",
      "| 에폭 12 |  반복 50 / 60 | 손실 0.59\n",
      "| 에폭 12 |  반복 60 / 60 | 손실 0.60\n",
      "| 에폭 13 |  반복 10 / 60 | 손실 0.59\n",
      "| 에폭 13 |  반복 20 / 60 | 손실 0.59\n",
      "| 에폭 13 |  반복 30 / 60 | 손실 0.58\n",
      "| 에폭 13 |  반복 40 / 60 | 손실 0.58\n",
      "| 에폭 13 |  반복 50 / 60 | 손실 0.56\n",
      "| 에폭 13 |  반복 60 / 60 | 손실 0.56\n",
      "| 에폭 14 |  반복 10 / 60 | 손실 0.56\n",
      "| 에폭 14 |  반복 20 / 60 | 손실 0.56\n",
      "| 에폭 14 |  반복 30 / 60 | 손실 0.55\n",
      "| 에폭 14 |  반복 40 / 60 | 손실 0.53\n",
      "| 에폭 14 |  반복 50 / 60 | 손실 0.53\n",
      "| 에폭 14 |  반복 60 / 60 | 손실 0.54\n",
      "| 에폭 15 |  반복 10 / 60 | 손실 0.54\n",
      "| 에폭 15 |  반복 20 / 60 | 손실 0.51\n",
      "| 에폭 15 |  반복 30 / 60 | 손실 0.53\n",
      "| 에폭 15 |  반복 40 / 60 | 손실 0.51\n",
      "| 에폭 15 |  반복 50 / 60 | 손실 0.51\n",
      "| 에폭 15 |  반복 60 / 60 | 손실 0.51\n",
      "| 에폭 16 |  반복 10 / 60 | 손실 0.51\n",
      "| 에폭 16 |  반복 20 / 60 | 손실 0.50\n",
      "| 에폭 16 |  반복 30 / 60 | 손실 0.50\n",
      "| 에폭 16 |  반복 40 / 60 | 손실 0.49\n",
      "| 에폭 16 |  반복 50 / 60 | 손실 0.48\n",
      "| 에폭 16 |  반복 60 / 60 | 손실 0.49\n",
      "| 에폭 17 |  반복 10 / 60 | 손실 0.49\n",
      "| 에폭 17 |  반복 20 / 60 | 손실 0.48\n",
      "| 에폭 17 |  반복 30 / 60 | 손실 0.47\n",
      "| 에폭 17 |  반복 40 / 60 | 손실 0.47\n",
      "| 에폭 17 |  반복 50 / 60 | 손실 0.47\n",
      "| 에폭 17 |  반복 60 / 60 | 손실 0.47\n",
      "| 에폭 18 |  반복 10 / 60 | 손실 0.47\n",
      "| 에폭 18 |  반복 20 / 60 | 손실 0.47\n",
      "| 에폭 18 |  반복 30 / 60 | 손실 0.45\n",
      "| 에폭 18 |  반복 40 / 60 | 손실 0.46\n",
      "| 에폭 18 |  반복 50 / 60 | 손실 0.45\n",
      "| 에폭 18 |  반복 60 / 60 | 손실 0.46\n",
      "| 에폭 19 |  반복 10 / 60 | 손실 0.45\n",
      "| 에폭 19 |  반복 20 / 60 | 손실 0.46\n",
      "| 에폭 19 |  반복 30 / 60 | 손실 0.44\n",
      "| 에폭 19 |  반복 40 / 60 | 손실 0.44\n",
      "| 에폭 19 |  반복 50 / 60 | 손실 0.44\n",
      "| 에폭 19 |  반복 60 / 60 | 손실 0.44\n",
      "| 에폭 20 |  반복 10 / 60 | 손실 0.44\n",
      "| 에폭 20 |  반복 20 / 60 | 손실 0.45\n",
      "| 에폭 20 |  반복 30 / 60 | 손실 0.42\n",
      "| 에폭 20 |  반복 40 / 60 | 손실 0.43\n",
      "| 에폭 20 |  반복 50 / 60 | 손실 0.43\n",
      "| 에폭 20 |  반복 60 / 60 | 손실 0.43\n",
      "학습시간 : 29.50초\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "import time\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "max_epoch = 20\n",
    "batch_size = 1000\n",
    "hidden_size = 128\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 데이터 읽기, 모델과 옵티마이저 생성\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "model = TwoLayerNet(input_size=784, hidden_size=hidden_size, output_size=10)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "# 학습에 사용하는 변수\n",
    "data_size = len(x_train)\n",
    "max_iters = data_size // batch_size\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "loss_list = []\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(max_epoch):\n",
    "    #데이터 뒤섞기\n",
    "    permutation = np.random.permutation(x_train.shape[0])\n",
    "    x = x_train[permutation,:] if x_train.ndim == 2 else x_train[permutation,:,:,:]\n",
    "    t = t_train[permutation]\n",
    "\n",
    "    for iters in range(max_iters):\n",
    "        batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "        batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "        # 기울기를 구해 매개변수 갱신\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "        # 정기적으로 학습 경과 출력\n",
    "        if (iters+1) % 10 == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            print('| 에폭 %d |  반복 %d / %d | 손실 %.2f'\n",
    "                  % (epoch + 1, iters + 1, max_iters, avg_loss))\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0, 0\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(\"학습시간 : {0:.2f}초\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 10 / 60 | 손실 2.30\n",
      "| 에폭 1 |  반복 20 / 60 | 손실 2.30\n",
      "| 에폭 1 |  반복 30 / 60 | 손실 2.30\n",
      "| 에폭 1 |  반복 40 / 60 | 손실 2.29\n",
      "| 에폭 1 |  반복 50 / 60 | 손실 2.29\n",
      "| 에폭 1 |  반복 60 / 60 | 손실 2.29\n",
      "| 에폭 2 |  반복 10 / 60 | 손실 2.28\n",
      "| 에폭 2 |  반복 20 / 60 | 손실 2.28\n",
      "| 에폭 2 |  반복 30 / 60 | 손실 2.27\n",
      "| 에폭 2 |  반복 40 / 60 | 손실 2.27\n",
      "| 에폭 2 |  반복 50 / 60 | 손실 2.26\n",
      "| 에폭 2 |  반복 60 / 60 | 손실 2.25\n",
      "| 에폭 3 |  반복 10 / 60 | 손실 2.23\n",
      "| 에폭 3 |  반복 20 / 60 | 손실 2.21\n",
      "| 에폭 3 |  반복 30 / 60 | 손실 2.19\n",
      "| 에폭 3 |  반복 40 / 60 | 손실 2.17\n",
      "| 에폭 3 |  반복 50 / 60 | 손실 2.14\n",
      "| 에폭 3 |  반복 60 / 60 | 손실 2.10\n",
      "| 에폭 4 |  반복 10 / 60 | 손실 2.06\n",
      "| 에폭 4 |  반복 20 / 60 | 손실 2.02\n",
      "| 에폭 4 |  반복 30 / 60 | 손실 1.96\n",
      "| 에폭 4 |  반복 40 / 60 | 손실 1.92\n",
      "| 에폭 4 |  반복 50 / 60 | 손실 1.86\n",
      "| 에폭 4 |  반복 60 / 60 | 손실 1.80\n",
      "| 에폭 5 |  반복 10 / 60 | 손실 1.73\n",
      "| 에폭 5 |  반복 20 / 60 | 손실 1.68\n",
      "| 에폭 5 |  반복 30 / 60 | 손실 1.62\n",
      "| 에폭 5 |  반복 40 / 60 | 손실 1.56\n",
      "| 에폭 5 |  반복 50 / 60 | 손실 1.51\n",
      "| 에폭 5 |  반복 60 / 60 | 손실 1.45\n",
      "| 에폭 6 |  반복 10 / 60 | 손실 1.41\n",
      "| 에폭 6 |  반복 20 / 60 | 손실 1.36\n",
      "| 에폭 6 |  반복 30 / 60 | 손실 1.31\n",
      "| 에폭 6 |  반복 40 / 60 | 손실 1.27\n",
      "| 에폭 6 |  반복 50 / 60 | 손실 1.23\n",
      "| 에폭 6 |  반복 60 / 60 | 손실 1.19\n",
      "| 에폭 7 |  반복 10 / 60 | 손실 1.15\n",
      "| 에폭 7 |  반복 20 / 60 | 손실 1.12\n",
      "| 에폭 7 |  반복 30 / 60 | 손실 1.08\n",
      "| 에폭 7 |  반복 40 / 60 | 손실 1.07\n",
      "| 에폭 7 |  반복 50 / 60 | 손실 1.03\n",
      "| 에폭 7 |  반복 60 / 60 | 손실 1.01\n",
      "| 에폭 8 |  반복 10 / 60 | 손실 0.98\n",
      "| 에폭 8 |  반복 20 / 60 | 손실 0.95\n",
      "| 에폭 8 |  반복 30 / 60 | 손실 0.93\n",
      "| 에폭 8 |  반복 40 / 60 | 손실 0.92\n",
      "| 에폭 8 |  반복 50 / 60 | 손실 0.89\n",
      "| 에폭 8 |  반복 60 / 60 | 손실 0.88\n",
      "| 에폭 9 |  반복 10 / 60 | 손실 0.85\n",
      "| 에폭 9 |  반복 20 / 60 | 손실 0.84\n",
      "| 에폭 9 |  반복 30 / 60 | 손실 0.82\n",
      "| 에폭 9 |  반복 40 / 60 | 손실 0.81\n",
      "| 에폭 9 |  반복 50 / 60 | 손실 0.79\n",
      "| 에폭 9 |  반복 60 / 60 | 손실 0.77\n",
      "| 에폭 10 |  반복 10 / 60 | 손실 0.76\n",
      "| 에폭 10 |  반복 20 / 60 | 손실 0.75\n",
      "| 에폭 10 |  반복 30 / 60 | 손실 0.74\n",
      "| 에폭 10 |  반복 40 / 60 | 손실 0.72\n",
      "| 에폭 10 |  반복 50 / 60 | 손실 0.71\n",
      "| 에폭 10 |  반복 60 / 60 | 손실 0.70\n",
      "| 에폭 11 |  반복 10 / 60 | 손실 0.68\n",
      "| 에폭 11 |  반복 20 / 60 | 손실 0.68\n",
      "| 에폭 11 |  반복 30 / 60 | 손실 0.68\n",
      "| 에폭 11 |  반복 40 / 60 | 손실 0.67\n",
      "| 에폭 11 |  반복 50 / 60 | 손실 0.66\n",
      "| 에폭 11 |  반복 60 / 60 | 손실 0.64\n",
      "| 에폭 12 |  반복 10 / 60 | 손실 0.63\n",
      "| 에폭 12 |  반복 20 / 60 | 손실 0.64\n",
      "| 에폭 12 |  반복 30 / 60 | 손실 0.63\n",
      "| 에폭 12 |  반복 40 / 60 | 손실 0.61\n",
      "| 에폭 12 |  반복 50 / 60 | 손실 0.60\n",
      "| 에폭 12 |  반복 60 / 60 | 손실 0.60\n",
      "| 에폭 13 |  반복 10 / 60 | 손실 0.60\n",
      "| 에폭 13 |  반복 20 / 60 | 손실 0.58\n",
      "| 에폭 13 |  반복 30 / 60 | 손실 0.59\n",
      "| 에폭 13 |  반복 40 / 60 | 손실 0.56\n",
      "| 에폭 13 |  반복 50 / 60 | 손실 0.56\n",
      "| 에폭 13 |  반복 60 / 60 | 손실 0.57\n",
      "| 에폭 14 |  반복 10 / 60 | 손실 0.56\n",
      "| 에폭 14 |  반복 20 / 60 | 손실 0.55\n",
      "| 에폭 14 |  반복 30 / 60 | 손실 0.57\n",
      "| 에폭 14 |  반복 40 / 60 | 손실 0.54\n",
      "| 에폭 14 |  반복 50 / 60 | 손실 0.54\n",
      "| 에폭 14 |  반복 60 / 60 | 손실 0.53\n",
      "| 에폭 15 |  반복 10 / 60 | 손실 0.55\n",
      "| 에폭 15 |  반복 20 / 60 | 손실 0.53\n",
      "| 에폭 15 |  반복 30 / 60 | 손실 0.52\n",
      "| 에폭 15 |  반복 40 / 60 | 손실 0.51\n",
      "| 에폭 15 |  반복 50 / 60 | 손실 0.52\n",
      "| 에폭 15 |  반복 60 / 60 | 손실 0.49\n",
      "| 에폭 16 |  반복 10 / 60 | 손실 0.51\n",
      "| 에폭 16 |  반복 20 / 60 | 손실 0.51\n",
      "| 에폭 16 |  반복 30 / 60 | 손실 0.50\n",
      "| 에폭 16 |  반복 40 / 60 | 손실 0.50\n",
      "| 에폭 16 |  반복 50 / 60 | 손실 0.48\n",
      "| 에폭 16 |  반복 60 / 60 | 손실 0.48\n",
      "| 에폭 17 |  반복 10 / 60 | 손실 0.50\n",
      "| 에폭 17 |  반복 20 / 60 | 손실 0.49\n",
      "| 에폭 17 |  반복 30 / 60 | 손실 0.47\n",
      "| 에폭 17 |  반복 40 / 60 | 손실 0.47\n",
      "| 에폭 17 |  반복 50 / 60 | 손실 0.48\n",
      "| 에폭 17 |  반복 60 / 60 | 손실 0.47\n",
      "| 에폭 18 |  반복 10 / 60 | 손실 0.46\n",
      "| 에폭 18 |  반복 20 / 60 | 손실 0.47\n",
      "| 에폭 18 |  반복 30 / 60 | 손실 0.47\n",
      "| 에폭 18 |  반복 40 / 60 | 손실 0.46\n",
      "| 에폭 18 |  반복 50 / 60 | 손실 0.47\n",
      "| 에폭 18 |  반복 60 / 60 | 손실 0.46\n",
      "| 에폭 19 |  반복 10 / 60 | 손실 0.46\n",
      "| 에폭 19 |  반복 20 / 60 | 손실 0.46\n",
      "| 에폭 19 |  반복 30 / 60 | 손실 0.45\n",
      "| 에폭 19 |  반복 40 / 60 | 손실 0.45\n",
      "| 에폭 19 |  반복 50 / 60 | 손실 0.44\n",
      "| 에폭 19 |  반복 60 / 60 | 손실 0.45\n",
      "| 에폭 20 |  반복 10 / 60 | 손실 0.44\n",
      "| 에폭 20 |  반복 20 / 60 | 손실 0.45\n",
      "| 에폭 20 |  반복 30 / 60 | 손실 0.43\n",
      "| 에폭 20 |  반복 40 / 60 | 손실 0.43\n",
      "| 에폭 20 |  반복 50 / 60 | 손실 0.43\n",
      "| 에폭 20 |  반복 60 / 60 | 손실 0.44\n",
      "학습시간 : 12.08초\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "import numpy as np\n",
    "import cupy\n",
    "import time\n",
    "\n",
    "def convert_cupy(x):\n",
    "    return cupy.asarray(x)\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "\n",
    "        W1 = cupy.asarray(0.01 * np.random.randn(I, H))\n",
    "        b1 = cupy.asarray(np.zeros(H))\n",
    "        W2 = cupy.asarray(0.01 * np.random.randn(H, O))\n",
    "        b2 = cupy.asarray(np.zeros(O))\n",
    "\n",
    "        self.layers = [Affine(W1, b1), Sigmoid(), Affine(W2, b2)]\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        score = self.predict(x)\n",
    "        loss = self.loss_layer.forward(score, t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [cupy.zeros_like(W), cupy.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = cupy.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = cupy.dot(dout, W.T)\n",
    "        dW = cupy.dot(self.x.T, dout)\n",
    "        db = cupy.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "max_epoch = 20\n",
    "batch_size = 1000\n",
    "hidden_size = 128\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 데이터 읽기, 모델과 옵티마이저 생성\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "model = TwoLayerNet(input_size=784, hidden_size=hidden_size, output_size=10)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "# 학습에 사용하는 변수\n",
    "data_size = len(x_train)\n",
    "max_iters = data_size // batch_size\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "loss_list = []\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(max_epoch):\n",
    "    #데이터 뒤섞기\n",
    "    permutation = np.random.permutation(x_train.shape[0])\n",
    "    x = x_train[permutation,:] if x_train.ndim == 2 else x_train[permutation,:,:,:]\n",
    "    t = t_train[permutation]\n",
    "\n",
    "    for iters in range(max_iters):\n",
    "        batch_x = convert_cupy(x[iters*batch_size:(iters+1)*batch_size])\n",
    "        batch_t = convert_cupy(t[iters*batch_size:(iters+1)*batch_size])\n",
    "        \n",
    "\n",
    "        # 기울기를 구해 매개변수 갱신\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "        # 정기적으로 학습 경과 출력\n",
    "        if (iters+1) % 10 == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            print('| 에폭 %d |  반복 %d / %d | 손실 %.2f'\n",
    "                  % (epoch + 1, iters + 1, max_iters, avg_loss))\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0, 0\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(\"학습시간 : {0:.2f}초\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 조건\n",
    "EPOCH = 20<br>\n",
    "BATCH_SIZE = 1000<br>\n",
    "HIDDEN_NODE = 128<br>\n",
    "\n",
    "CPU와 GPU 모두 동일한 조건으로 테스트 하였습니다.<br>\n",
    "\n",
    "---\n",
    "### 학습 시 사양\n",
    "CPU = E3-1231V<br>\n",
    "GPU = GTX970<br>\n",
    "RAM = 16GB<br>\n",
    "\n",
    "---\n",
    "### 학습 시간\n",
    "CPU = 약 30초<br>\n",
    "GPU = 약 12초<br>\n",
    "\n",
    "이번 예제에서는 GPU로 학습 시 약 2배 이상 빨라졌습니다.<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
