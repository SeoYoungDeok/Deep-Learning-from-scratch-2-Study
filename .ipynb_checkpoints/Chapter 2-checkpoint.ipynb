{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 [자연어와 단어의 분산 표현]\n",
    "---\n",
    "이번 장부터 자연어 처리를 학습니다.<br>\n",
    "자연어 처리의 핵심은 컴퓨터가 우리의 말을 알아듣게(이해하게) 만드는 것입니다.<br>\n",
    "컴퓨터에 말을 이해시킨다는 것이 무슨 뜻인지, 그리고 어떤 방법들이 존재하는지를 중심으로 학습하겠습니다.<br>\n",
    "앞으로 사용할 함수와 각 계층은 클래스로 구현하여 common 폴더에 저장하였습니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 자연어 처리\n",
    "<b>자연어</b><sup>natural language</sup>란 한국어와 영어 등 우리가 평소에 쓰는 말을 뜻합니다.<br>\n",
    "<b>자연어 처리</b><sup>Natural Language Processing</sup>(<b>NLP</b>)는 '우리의 말을 컴퓨터에게 이해시키기 위한 기술(분야)'입니다.<br>\n",
    "<br>\n",
    "하지만 컴퓨터가 이해할 수 있는 언어라고 하면 '<b>프로그래밍 언어</b>'같은 것이 떠오릅니다.<br>\n",
    "이러한 언어는 의미를 고유하게 해석할 수 있도록 문법이 정의되어 있고, 컴퓨터는 <b>정해진 규칙</b>에 따라서 언어를 해석합니다.<br>\n",
    "<br>\n",
    "일반적인 <b>프로그래밍 언어</b>는 기계적이고 고정되어있는 '<b>딱딱한 언어</b>'입니다.<br>\n",
    "반면, 한국어나 영어 같은 <b>자연어</b>는 '<b>부드러운 언어</b>'입니다.<br>\n",
    "'<b>부드럽다</b>'라는 것은 똑같은 의미의 문장도 여러 형태로 표현할 수 있다거나, 문장의 뜻이 <b>애매할 수 있다거나</b><br>\n",
    "그 의미나 형태가 <b>유연하게 바뀐다</b>는 뜻입니다. 세월이 흐르면서 <b>새로운 말이나 새로운 의미가 생겨나</b>거나 있던 것이 사라지기도 하죠.<br>\n",
    "이 모두가 자연어가 <b>부드럽기 때문입니다</b>.<br>\n",
    "<br>\n",
    "이처럼 유연하고 부드러운 자연어를 컴퓨터에게 이해시키기란 어려운 도전입니다.<br>\n",
    "하지만 그 난제를 해결할 수 있다면, 수많은 사람에게 도움되는 일을 컴퓨터에게 시킬 수 있습니다.<br>\n",
    "\n",
    "### 1.1 단어의 의미\n",
    "우리의 말은 '문자'로 구성되며, 말의 의미는 '<b>단어</b>'로 구성됩니다.<br>\n",
    "단어는 의미의 <b>최소단위</b>인 셈입니다.<br>\n",
    "그래서 자연어를 컴퓨터에게 이해시키는 데는 무엇보다 '단어의 의미'를 이해시키는 게 중요합니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 시소러스\n",
    "시소러스란 (기본적으로는) 유의어 사전으로, '뜻이 같은 단어(동의어)'나 '뜻이 비슷한 단어(유의어)'가 한 그룹으로 분류되어 있습니다.<br>\n",
    "<img src=img/fig2-1.png width='400'> <img src=img/fig2-2.png width='400'>\n",
    "이처럼 모든 단어에 대한 유의어 집합을 만든다음, 단어들의 관계를 그래프로 표현하여 단어사이의 연결을 정의할 수 있습니다.<br>\n",
    "그러면 이 '단어 네트워크'를 이용하여 컴퓨터에게 단어 사이의 관계를 가르칠 수 있습니다.<br>\n",
    "\n",
    "### 2.1 시소러스의 문제점\n",
    "시소러스를 이용하면 '단어의 의미'를 (간접적으로라도) 컴퓨터에 전달할 수 있습니다.<br>\n",
    "하지만 이처럼 사람이 수작업으로 레이블링하는 방식에는 크나큰 결점이 존재합니다.<br>\n",
    "* 시대 변화에 대응하기 어렵다.\n",
    "* 사람을 쓰는 비용은 크다.\n",
    "* 단어의 미묘한 차이를 표현할 수 없다.<br>\n",
    "\n",
    "이러한 문제를 피하기 위해, '<b>통계 기반 기법</b>'과 신경망을 사용한 '<b>추론 기반 기법</b>'을 알아볼 것입니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 통계 기반 기법\n",
    "지금부터 <b>말뭉치</b><sup>corpus</sup>를 이용하여 통계 기반 기법을 살펴보겠습니다.<br>\n",
    "말뭉치란 대량의 텍스트 데이터입니다. 그 안에 담긴 문장들은 사람이 쓴 글이고, 자연어에 대한 사람의 '지식'이 충분히 담겨 있다고 볼 수 있습니다.<br>\n",
    "문장을 쓰는 방법, 단어를 선택하는 방법, 단어의 의미 등 사람이 알고 있는 자연어에 대한 지식이 포함되어 있는 것입니다.<br>\n",
    "통계 기반 기법의 목표는 이 처럼 사람의 지식으로 가득한 말뭉치에서 자동으로, 그리고 효율적으로 그 핵심을 추출하는 것입니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 파이썬으로 말뭉치 전처리하기\n",
    "이번 절에서는 아래와 같은 문장 하나로 이루어진 텍스트를 말뭉치로 사용하겠습니다.<br>\n",
    "``` python\n",
    "text = 'You say goodbye and I say hello.'\n",
    "```\n",
    "실전이라면 이 text에 수천, 수만 개가 넘는 문장이 (연이어) 담겨 있을 것입니다.<br>\n",
    "하지만 지금은 쉽게 설명하기 위해 이 작은 텍스트 데이터만으로 전처리를 수행하겠습니다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text = you say goodbye and i say hello .\n",
      "words = ['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']\n"
     ]
    }
   ],
   "source": [
    "# text를 단어 단위로 분할\n",
    "text = 'You say goodbye and I say hello.' \n",
    "\n",
    "text = text.lower()\n",
    "text = text.replace('.', ' .')\n",
    "print('text = {}'.format(text))\n",
    "\n",
    "words = text.split(' ')\n",
    "print('words = {}'.format(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 먼저 lower() 메서드를 사용하여 모든 문자를 소문자로 변환했습니다.<br>\n",
    "문장 첫머리의 대문자로 시작하는 단어와 소문자 단어를 똑같이 취급하기 위함입니다.<br>\n",
    "그리고 split(' ')메서드를 이용하여 공백을 기준으로 분할했습니다.<br>\n",
    "<br>\n",
    "이제 원래의 문장을 '단어 목록' 형태로 이용할 수 있게 되었습니다.<br>\n",
    "하지만 단어를 텍스트 그대로 조작하기란 여러 면에서 불편합니다.<br>\n",
    "그래서 단어에 ID를 부여하고, ID의 리스트로 이용할 수 있도록 한 번 더 손질합니다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_to_word : {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "word_to_id : {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "\n",
      "id_to_word[2] : goodbye\n",
      "word_to_id['and'] : 3\n"
     ]
    }
   ],
   "source": [
    "# 단어와 ID사전 만들기\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word\n",
    "\n",
    "print('id_to_word : {}'.format(id_to_word))\n",
    "print('word_to_id : {}'.format(word_to_id))\n",
    "print()\n",
    "print('id_to_word[2] : {}'.format(id_to_word[2]))\n",
    "print('word_to_id[\\'and\\'] : {}'.format(word_to_id['and']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'단어 목록' words의 요소를 처음부터 끝까지 순회하며 단어를 가져옵니다.<br>\n",
    "처음 들어오는 단어라면 word_to_id 사전(딕셔너리)의 마지막 인덱스를 가져와서 new_id에 저장합니다.<br>\n",
    "word_to_id 사전에는 해당 단어에 id를 맵핑합니다.<br>\n",
    "id_to_word 사전에는 해당 id에 단어를 맵핑합니다.<br>\n",
    "이것으로 단어 ID를 이용하여 단어를 검색하거나, 반대로 단어를 이용하여 단어 ID를 검색할 수 있습니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 '단어 목록'을 '단어 ID 목록'으로 변경해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것으로 말뭉치를 다룰 준비를 마쳤습니다.<br>\n",
    "이상의 처리를 한 데 모아 preprocess()라는 함수로 구현하여 common/util.py에 저장했습니다.<br>\n",
    "이제 '통계 기반 기법'을 사용하여 단어를 벡터로 표현해보겠습니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 단어의 분산 표현\n",
    "'색'을 벡터로 표현하듯 '단어'도 벡터로 표현할 수 있을까요?<br>\n",
    "우리가 원하는 것은 '단어의 의미'를 정확하게 파악할 수 있는 벡터 표현입니다.<br>\n",
    "이를 자연어 처리 분야에서는 단어의 <b>분산표현</b><sup>distributional representation</sup>이라고 합니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 분포 가설\n",
    "자연어 처리의 역사에서 단어를 벡터로 표현하는 연구는 수럾이 이뤄져 왔습니다.  \n",
    "그 연구들의 중요한 기법은 간단한 아이디어에 뿌리를 두고 있음을 알 수 있습니다.  \n",
    "그 아이디어는 '단어의 의미는 주변 단어에 의해 형성된다.'라는 것입니다.  \n",
    "이를 **분포가설**<sup>distributional hypothesis</sup>이라고 합니다.  \n",
    "  \n",
    "단어 자체에는 의미가 없고, 그 단어가 사용된 '맥락<sup>context</sup>'이 의미를 형성한다는 것입니다.  \n",
    "예컨대 \"I drink beer\"와 \"We drink wine\"처럼 \"drink\"의 주변에는 음료가 등장하기 쉬울 것입니다.  \n",
    "  \n",
    "앞으로 '맥락'이라는 말을 자주 사용할 겁니다. 이번 장에서 '맥락'이라 하면 (주목하는 단어)주변에 놓인 단어를 가리킵니다.  \n",
    "그림 2-3에서는 좌우의 각 두 단어씩이 '맥락'에 해당합니다.  \n",
    "<img src=img/fig2-3.png width='600'>\n",
    "그리고 맥락의 크기 (주변 단어를 몇 개나 포함할지)를 '윈도우 크기<sup>window size</sup>'라고 합니다.  \n",
    "윈도우 크기가 1이면 좌우 한 단어씩이, 윈도우 크기가 2이면 좌우 두 단어씩이 맥락에 포함됩니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 동시발생 행렬\n",
    "그러면 분포 가설에 기초해 단어를 벡터로 나타내는 방법을 생각해봅시다.  \n",
    "주변 단어를 '세어보는'방법이 자연스럽게 떠오를 것입니다.  \n",
    "어떤 단어에 주목했을 때, 그 주변에 어떤단어가 몇 번이나 등장하는지를 세어 집계하는 방법입니다.  \n",
    "이 책에서는 이를 '통계 기반<sup>statistical based</sup>'기법이라고 하겠습니다.  \n",
    "  \n",
    "먼저 '3.1 파이썬으로 말뭉치 전처리하기'에서 봤던 말뭉치와 preprocess() 함수를 사용해 전처리하는 일부터 시작하겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import preprocess\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "print(corpus)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로는 각 단어의 맥락에 해당하는 단어의 빈도를 세어 보겠습니다.  \n",
    "읜도우 크기는 1로 하고, 단어 ID가 0인 \"you\"부터 시작해보죠.  \n",
    "<img src=img/fig2-4.png width='500'> <img src=img/fig2-5.png width='500'>\n",
    "그림 2-5는 단어 \"you\"의 맥락으로써 동시에 발생(등장)하는 단어의 빈도를 나타낸 것입니다.  \n",
    "그리고 이를 바탕으로 \"you\"라는 단어를 [0. 1. 0. 0. 0. 0. 0]이라는 벡터로 표현할 수 있습니다.  \n",
    "이처럼 모든 단어에 대해 동시발생하는 단어를 표로 정리해보겠습니다.  \n",
    "<img src=img/fig2-7.png width='500'>\n",
    "이 표의 각 행은 해당 단어를 표현한 벡터가 됩니다.  \n",
    "이 표가 행렬의 형태를 띤다는 뜻에서 **동시발생 행렬**<sup>co-occurrence matrix</sup>이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동시발생 행렬을 자동으로 만들어주는 함수를 common/util.py에 저장했습니다.  \n",
    "함수이름은 create_co_matrix(corpus, vocab_size, window_size=1)로 했습니다.  \n",
    "인수들은 차례로 단어 ID리스트, 어휘 수, 윈도우 크기를 나타냅니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import preprocess\n",
    "\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 벡터 간 유사도\n",
    "앞에서 동시발생 행렬을 활용해 단어를 벡터로 표현하는 방법을 알아봤습니다.  \n",
    "계속해서 벡터 사이의 유사도를 측정하는 방법을 살펴보겠습니다.  \n",
    "  \n",
    "단어 벡터의 유사도를 나타낼 때는 **코사인 유사도**<sup>cosine similarity</sup>를 자주 이용합니다.  \n",
    "두 벡터 $x = \\left(x_1, x_2, x_3, \\cdots, x_n \\right)$과 $y = \\left(y_1, y_2, y_3, \\cdots, y_n \\right)$이 있다면, 코사인 유사도는 다음 식으로 정의됩니다.  \n",
    "$$similarity(x,y)=\\frac{x \\cdot y}{\\lVert x \\rVert\\lVert y \\rVert}=\\frac{x_1y_1+\\cdots+x_ny_n}{\\sqrt{x^2_1+\\cdots+x^2_n}\\sqrt{y^2_1+\\cdots+y^2_n}}$$  \n",
    "식을보면 분자에는 벡터의 내적이, 분모에는 각 벡터의 노름<sup>norm</sup>이 등장합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 벡터 x와 y를 정규화를 합니다.  \n",
    "벡터 정규화란 벡터의 크기를 1로 만들어 단위 벡터가 되게 하는 것입니다.(x / (np.sqrt(np.sum(x ** 2)) + eps))  \n",
    "단위 벡터란 벡터의 각 성분을 크기로 나눈 벡터입니다.  \n",
    "  \n",
    "그리고 정규화한 두 벡터의 내적을 구합니다.  \n",
    "그 값이 0보다 크면 두 벡터 간의 각도는 90도 보다 작아집니다.  \n",
    "반대로 0보다 작아지면 두 벡터 간의 각도는 90도 보다 커집니다.  \n",
    "  \n",
    "따라서 코사인 유사도는 '두 벡터가 가리키는 방향이 얼마나 비슷한가'입니다.  \n",
    "두 벡터의 방향이 같다면 코사인 유사도가 1이 되며, 완전히 반대라면 -1이 됩니다.  \n",
    "(eps는 아주 작은 값으로 '0으로 나누기'오류를 막아줍니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067691154799\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import preprocess, create_co_matrix\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['you']]\n",
    "c1 = C[word_to_id['i']]\n",
    "\n",
    "print(cos_similarity(c0, c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드는 \"you\"와 \"i\"의 유사도를 구하는 코드입니다.  \n",
    "실행 결과 두 단어의 유사도는 0.70으로 나왔습니다.  \n",
    "즉 두 단어는 유사성이 크다고 말할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 유사 단어의 랭킹 표시\n",
    "코사인 유사도를 활용하여 어떤 단어가 검색어로 주어지면, 그 검색어와 비슷한 단어를 유사도 순으로 출력하는 함수를 만들어보겠습니다.  \n",
    "함수 이름은 most_similar()로 하고, 다음 인수들을 입력받도록 구현해보겠습니다.  \n",
    "  \n",
    "`most_similar(query, word_to_id, id_to_word, word_matrix, top=5)`\n",
    "  \n",
    "| 인수명 | 설명 |\n",
    "|:---|:---|\n",
    "| query | 검색어(단어) |\n",
    "| word_to_id | 단어에서 단어 ID로의 딕셔너리 |\n",
    "| id_to_word | 단어 ID에서 단어로의 딕셔너리 |\n",
    "| word_matrix | 단어 벡터들을 한데 모은 행렬, 각 행에는 대응하는 단어의 벡터가 저장되어 있다고 가정한다. |\n",
    "| top | 상위 몇 개까지 출력할지 설정 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    if query not in word_to_id:\n",
    "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067691154799\n",
      " i: 0.7071067691154799\n",
      " hello: 0.7071067691154799\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import preprocess, create_co_matrix, most_similar\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 \"you\"와 유사한 단어를 상위 5개만 출력했습니다.  \n",
    "결과를 봤을 때 \"i\"와 \"you\" 모두 인칭대명사이므로 둘이 비슷하다는 건 납득이 됩니다.  \n",
    "하지만 \"goodbye\"와 \"hello\"의 코사인 유사도가 높다는 것은 우리의 직관과는 거리가 멉니다.  \n",
    "이 문제는 말뭉치의 크기가 너무 작다는 것이 원인입니다.  \n",
    "따라서 나중에 더 큰 말뭉치를 사용하여 똑같이 실험해 보겠습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 통계 기반 기법 개선하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 상호정보량\n",
    "앞 절에서 본 동시발생 행렬의 원소는 두 단어가 동시에 발생한 횟수를 나타냅니다.  \n",
    "그러나 이 '발생'횟수라는 것은 사실 그리 좋은 특징이 아닙니다.  \n",
    "\n",
    "예컨대 말뭉치에서 \"the\"와 \"car\"의 동시발생을 생각해보면 분명\"... the car ...\"라는 문구가 자주 출현할 것입니다.  \n",
    "따라서 두 단어의 동시발생 횟수는 아주 많겠죠. 한편, \"car\"와 \"drive\"는 확실히 관련성이 높습니다.  \n",
    "하지만 단순히 등장 횟수만을 본다면 \"car\"는 \"drive\"보다는 \"the\"와 관련성이 높다고 나올 것입니다.  \n",
    "\n",
    "이러한 문제를 해결하시 위해 **점멸 상호정보향**<sup>Pointwise Mutual Information</sup>(PMI)이라는 척도를 사용합니다.  \n",
    "PMI는 확률 변수 x와 y에 대해 다음 식으로 정의됩니다.  \n",
    "  \n",
    "$$ PMI(x,y)=log_2\\frac{P(x,y)}{P(x)P(y)} $$\n",
    "  \n",
    "식에서 $P(x)$는 $x$가 일어날 확융, $P(y)$는 $y$가 일어날 확률, $P(x,y)$는 $x$와 $y$가 동시에 일어날 확률을 뜻합니다.  \n",
    "이 PMI 값이 높을수록 관련성이 높다는 의미입니다.  \n",
    "\n",
    "이 식을 자연어 예에 적용하면 $P(x)$는 단어$x$가 말뭉치에 등장할 확률을 가리킵니다.  \n",
    "예컨대 10,000개의 단어로 이뤄진 말뭉치에서 \"the\"가 100번 등장한다면 $P(\"the\")=\\frac{100}{10000}=0.01$이 됩니다.  \n",
    "또한 $P(x,y)$는 단어 $x$와 $y$가 동시발생할 확률이므로, 마찬가지로 \"the\"와 \"car\"가 10번 동시발생했다면 $P(\"the\",\"car\")=\\frac{10}{10000}=0.001$이 되는 것이죠.  \n",
    "  \n",
    "그럼 동시발생 행렬을 사용하여 식을 다시 써보겠습니다. $C$는 동시발생 행렬, $C(x,y)$는 단어 $x$와 $y$가 동시발생하는 횟수  \n",
    "$C(x)$와 $C(y)$는 각각 단어 $x$와 $y$의 등장 횟수입니다. 이때 말뭉치에 포함된 단어 수를 $N$이라 하면, 식은 다음과 같이 변합니다.\n",
    "  \n",
    "$$ PMI(x,y)=log_2\\frac{P(x,y)}{P(x)P(y)}=log_2\\frac{\\frac{C(x,y)}{N}}{\\frac{C(x)}{N}\\frac{C(y)}{N}}=log_2\\frac{C(x,y) \\cdot N}{C(x)C(y)}$$\n",
    "  \n",
    "그러면 위 식을 이용해 구체적인 계산을 해봅시다.  \n",
    "말뭉치의 단어 수$(N)$를 10,000이라 하고, \"the\"와 \"car\"와 \"drive\"가 각 1,000번, 20번, 10번 등장했다고 합시다.  \n",
    "그리고 \"the\"와 \"car\"의 동시발생 수는 10회, \"car\"와 \"drive\"의 동시발생 수는 5회라고 가정합시다.  \n",
    "이 조건이라면, 동시발생 횟수 관점에서는 \"car\"는 \"drive\"보다 \"the\"와 관련이 깊다고 나올 것입니다.  그렇다면 PMI관점에서는 어떨까요?  \n",
    "  \n",
    "$$ PMI(\"the\",\"car\")=log_2\\frac{10 \\cdot 10000}{1000 \\cdot 20}\\approx2.32 $$\n",
    "  \n",
    "$$ PMI(\"car\",\"drive\")=log_2\\frac{5 \\cdot 10000}{20 \\cdot 10}\\approx7.97 $$\n",
    "  \n",
    "위 결과에서 알 수 있듯이 PMI를 이용하면 \"car\"는 \"the\"보다 \"drive\"와의 관련성이 더 높아집니다.  \n",
    "이러한 결과가 나온 이유는 단어가 단독으로 출현하는 횟가 고려되었기 때문입니다.  \n",
    "  \n",
    "하지만 이 PMI에도 한 가지 문제가 있습니다. 바로 두 단어의 동시발생 횟수가 0이면 $log_2 0 = -\\infty$가 된다는 점입니다.  \n",
    "이 문제를 피해기 위해 실제로 구현할 때는 **양의 상호정보량**<sup>Positive PMI</sup>(**PPMI**)을 사용합니다.  \n",
    "  \n",
    "$$ PPMI(x,y)=max(0,PMI(x,y)) $$\n",
    "  \n",
    "이 함수의 이름은 `ppmi(C, verbose=False, esp=1e-8)`로 짓겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(C, verbose=False, eps = 1e-8):\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100) == 0:\n",
    "                    print('%.1f%% 완료' % (100*cnt/total))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생 행렬\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import preprocess, create_co_matrix, most_similar, ppmi\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print('동시발생 행렬')\n",
    "print(C)\n",
    "print('-' * 50)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것으로 동시발생 행렬을 PPMI 행렬로 변환하는 법을 알아봤습니다.  \n",
    "  \n",
    "하지만 PPMI 행렬에도 여전히 큰 문제가 있습니다.  \n",
    "말뭉치의 어휘 수가 증가함에 따라 각 단어 벡터의 차원 수도 증가한다는 문제입니다.  \n",
    "예를 들어 말뭉치의 어휘 수가 10만 개라면 그 벡터의 차원 수도 똑같이 10만이 됩니다.  \n",
    "  \n",
    "또한, 이 행렬의 내용을 들여다보면 원소의 대부분이 0인 것을 알 수 있습니다.  \n",
    "벡터의 원소 대부분이 중요하지 않다는 뜻입니다.\n",
    "  \n",
    "이러한 문제에 대처하고자 자주 수행하는 기법이 바로 벡터의 차원 감소입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 차원 감소\n",
    "**차원감소**<sup>dimensionality reduction</sup>는 문자 그대로 벡터의 차원을 줄이는 방법을 말합니다.  \n",
    "그러나 단순히 줄이기만 하는 게 아니라, '중요한 정보'는 최대한 유지하면서 줄이는 게 핵심입니다.  \n",
    "  \n",
    "차원을 감소시키는 방법은 여러 가지가 있지만, 여기서는 **특잇값분해**<sup>Singular Value Decomposition</sup>(**SVD**)를 이용하겠습니다.  \n",
    "SVD는 임의의 행렬을 세 행렬의 곱으로 분해하며, 수식으로는 다음과 같습니다.  \n",
    "  \n",
    "$$ X = USV^T $$\n",
    "  \n",
    "위 식과 같이 SVD는 임의의 행렬 $X$를 $U, S, V$라는 세 행렬의 곱으로 분해합니다.\n",
    "아래 그림은 이 수식을 시각적으로 표현한 것입니다.  \n",
    "<img src=img/fig2-9.png width='500'> <img src=img/fig2-10.png width='500'>\n",
    "  \n",
    "단어의 PPMI행렬에 적용해본다면, 행렬 $X$의 각 행에는 해당 단어 ID의 단어 벡터가 저장되어 있으며  \n",
    "그 단어 벡터가 행렬 $U'$라는 차원 감소된 벡터로 표현되는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 SVD에 의한 차원 감소\n",
    "SVD는 넘파이의 linalg 모듈이 제공하는 svd 메서드로 실행할 수 있습니다.  \n",
    "그럼, 동시발생 행렬을 만들어 PPMI 행렬로 변환한 다음 SVD를 적용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생 행렬 : [0 1 0 0 0 0 0]\n",
      "PPMI 행렬 : [0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "SVD : [-3.409e-01 -1.110e-16 -3.886e-16 -1.205e-01  0.000e+00  9.323e-01\n",
      "  2.226e-16]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from common.util import preprocess, create_co_matrix, most_similar, ppmi\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "U, S, V = np.linalg.svd(W)\n",
    "\n",
    "print('동시발생 행렬 : {}'.format(C[0]))\n",
    "print('PPMI 행렬 : {}'.format(W[0]))\n",
    "print('SVD : {}'.format(U[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 결과에서 보듯 희소벡터인 W[0]가 SVD에 의해서 밀집벡터 U[0]로 변했습니다.  \n",
    "그리고 이 밀집벡터의 차원을 2차원 벡터로 줄이려면 처음 두 원소를 꺼내면 됩니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.409e-01 -1.110e-16]\n"
     ]
    }
   ],
   "source": [
    "print(U[0, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "    \n",
    "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그래프를 보면 \"goodbey\"와 \"hello\", \"you\"와 \"i\"가 제법 가까기 있음을 알 수 있습니다.(\"goodbey\"와 \"i\"가 겹쳐있음)  \n",
    "그러면 계속해서 PTB 데이터셋이라는 더 큰 말뭉치를 사용하여 똑같은 실험을 수행해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 PTB 데이터셋\n",
    "**펜 트리뱅크**<sup>Penn Treebank</sup>라고 하는 PTB 말뭉치는 주어진 기법의 품질을 측정하는 벤치마크로 자주 이용됩니다.  \n",
    "이 책에서도 PTB 말뭉치를 이용하여 다양한 실험을 수행합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ptb.train.txt ... \n",
      "Done\n",
      "말뭉치 크기: 929589\n",
      "corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "id_to_word[0]: aer\n",
      "id_to_word[1]: banknote\n",
      "id_to_word[2]: berlitz\n",
      "\n",
      "word_to_id['car']: 3856\n",
      "word_to_id['happy']: 4428\n",
      "word_to_id['lexus']: 7426\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from dataset import ptb\n",
    "\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "\n",
    "print('말뭉치 크기:', len(corpus))\n",
    "print('corpus[:30]:', corpus[:30])\n",
    "print()\n",
    "print('id_to_word[0]:', id_to_word[0])\n",
    "print('id_to_word[1]:', id_to_word[1])\n",
    "print('id_to_word[2]:', id_to_word[2])\n",
    "print()\n",
    "print(\"word_to_id['car']:\", word_to_id['car'])\n",
    "print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
    "print(\"word_to_id['lexus']:\", word_to_id['lexus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 PTB 데이터셋 평가\n",
    "PTB 데이터셋에 통계 기반 기법을 적용해보겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생 수 계산 ...\n",
      "PPMI 계산 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dud19\\Deep Learning from scratch 2\\common\\util.py:141: RuntimeWarning: overflow encountered in long_scalars\n",
      "  pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
      "C:\\Users\\dud19\\Deep Learning from scratch 2\\common\\util.py:141: RuntimeWarning: invalid value encountered in log2\n",
      "  pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0% 완료\n",
      "2.0% 완료\n",
      "3.0% 완료\n",
      "4.0% 완료\n",
      "5.0% 완료\n",
      "6.0% 완료\n",
      "7.0% 완료\n",
      "8.0% 완료\n",
      "9.0% 완료\n",
      "10.0% 완료\n",
      "11.0% 완료\n",
      "12.0% 완료\n",
      "13.0% 완료\n",
      "14.0% 완료\n",
      "15.0% 완료\n",
      "16.0% 완료\n",
      "17.0% 완료\n",
      "18.0% 완료\n",
      "19.0% 완료\n",
      "20.0% 완료\n",
      "21.0% 완료\n",
      "22.0% 완료\n",
      "23.0% 완료\n",
      "24.0% 완료\n",
      "25.0% 완료\n",
      "26.0% 완료\n",
      "27.0% 완료\n",
      "28.0% 완료\n",
      "29.0% 완료\n",
      "30.0% 완료\n",
      "31.0% 완료\n",
      "32.0% 완료\n",
      "33.0% 완료\n",
      "34.0% 완료\n",
      "35.0% 완료\n",
      "36.0% 완료\n",
      "37.0% 완료\n",
      "38.0% 완료\n",
      "39.0% 완료\n",
      "40.0% 완료\n",
      "41.0% 완료\n",
      "42.0% 완료\n",
      "43.0% 완료\n",
      "44.0% 완료\n",
      "45.0% 완료\n",
      "46.0% 완료\n",
      "47.0% 완료\n",
      "48.0% 완료\n",
      "49.0% 완료\n",
      "50.0% 완료\n",
      "51.0% 완료\n",
      "52.0% 완료\n",
      "53.0% 완료\n",
      "54.0% 완료\n",
      "55.0% 완료\n",
      "56.0% 완료\n",
      "57.0% 완료\n",
      "58.0% 완료\n",
      "59.0% 완료\n",
      "60.0% 완료\n",
      "61.0% 완료\n",
      "62.0% 완료\n",
      "63.0% 완료\n",
      "64.0% 완료\n",
      "65.0% 완료\n",
      "66.0% 완료\n",
      "67.0% 완료\n",
      "68.0% 완료\n",
      "69.0% 완료\n",
      "70.0% 완료\n",
      "71.0% 완료\n",
      "72.0% 완료\n",
      "73.0% 완료\n",
      "74.0% 완료\n",
      "75.0% 완료\n",
      "76.0% 완료\n",
      "77.0% 완료\n",
      "78.0% 완료\n",
      "79.0% 완료\n",
      "80.0% 완료\n",
      "81.0% 완료\n",
      "82.0% 완료\n",
      "83.0% 완료\n",
      "84.0% 완료\n",
      "85.0% 완료\n",
      "86.0% 완료\n",
      "87.0% 완료\n",
      "88.0% 완료\n",
      "89.0% 완료\n",
      "90.0% 완료\n",
      "91.0% 완료\n",
      "92.0% 완료\n",
      "93.0% 완료\n",
      "94.0% 완료\n",
      "95.0% 완료\n",
      "96.0% 완료\n",
      "97.0% 완료\n",
      "98.0% 완료\n",
      "99.0% 완료\n",
      "100.0% 완료\n",
      "calculating SVD ...\n",
      "\n",
      "[query] you\n",
      " i: 0.654574990272522\n",
      " we: 0.6331467032432556\n",
      " someone: 0.5727645754814148\n",
      " anybody: 0.5672106146812439\n",
      " do: 0.5663542747497559\n",
      "\n",
      "[query] year\n",
      " earlier: 0.6614880561828613\n",
      " quarter: 0.649252712726593\n",
      " month: 0.6475235223770142\n",
      " last: 0.6154425144195557\n",
      " next: 0.5795693397521973\n",
      "\n",
      "[query] car\n",
      " auto: 0.578158974647522\n",
      " luxury: 0.5436641573905945\n",
      " corsica: 0.4834359288215637\n",
      " truck: 0.48314720392227173\n",
      " cars: 0.47771623730659485\n",
      "\n",
      "[query] toyota\n",
      " motor: 0.7029744386672974\n",
      " nissan: 0.6932392120361328\n",
      " lexus: 0.646404504776001\n",
      " motors: 0.5958426594734192\n",
      " mazda: 0.5816032290458679\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import most_similar, create_co_matrix, ppmi\n",
    "from dataset import ptb\n",
    "\n",
    "\n",
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print('동시발생 수 계산 ...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "print('PPMI 계산 ...')\n",
    "W = ppmi(C, verbose=True)\n",
    "\n",
    "print('calculating SVD ...')\n",
    "try:\n",
    "    # truncated SVD (빠르다!)\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
    "                             random_state=None)\n",
    "except ImportError:\n",
    "    # SVD (느리다)\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "\n",
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 보면, \"car\"의 관련어로 \"auto\"가 보입니다.  \n",
    "그리고 \"toyota\"와 관련어로 \"nissan\", \"lexus\"등 자동차 제조업체나 브랜드가 보입니다.  \n",
    "이처럼 단어의 의미 혹은 문법적인 관점에서 비슷한 단어들이 가까운 벡터로 나타났습니다.  \n",
    "(참고로 Truncated SVD는 무작위 수를 사용하므로 결과가 매번 다릅니다.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
