{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 [어텐션]\n",
    "---\n",
    "이번 장에서는 seq2seq를 한층 더 강력하게 해주는 어텐션 메커니즘에 대해 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 어텐션의 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq의 문제점\n",
    "---\n",
    "seq2seq에서는 Encoder가 시계열 데이터를 인코딩합니다.  \n",
    "이때 Encoder의 출력은 '고정 길이의 벡터'였습니다.  \n",
    "고정 길이의 벡터라 함은 입력 문장의 길이에 관계없이, 항상 같은 길이의 벡터로 변환한다는 뜻입니다.  \n",
    "<img src=img/fig8-1.png width='600'>  \n",
    "문장이 길어지면 결국 필요한 정보들을 벡터에 다 담지 못하게 됩니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 개선\n",
    "---\n",
    "위 문제점을 개선하는 방법으로 각 시각별 LSTM 계층의 은닉 상태 벡터를 모두 이용하는 것입니다.  \n",
    "<img src=img/fig8-2.png width='700'>  \n",
    "이렇게 각 시각의 은닉 상태 벡터를 모두 이용하면 입력된 데이터와 같은 수의 벡터를 얻을 수 있습니다.  \n",
    "위 예에서는 5개의 단어가 입력되었고, 이때 Encoder는 5개의 벡터를 출력합니다.  \n",
    "이것으로 Encoder는 '하나의 고정 길이 벡터'라는 제약으로부터 해방됩니다.  \n",
    "<img src=img/fig8-3.png width='700'>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 개선 ①\n",
    "---\n",
    "이전 Decoder에서는 Encoder의 마지막 은닉 상태 벡터만 들어왔습니다만, 위의 개선된 Encoder에서는 모든 은닉 상태 벡터 **hs**가 넘어옵니다.  \n",
    "이 **hs**를 전부 활용할 수 있도록 Decoder를 개선 시키겠습니다.  \n",
    "\n",
    "그 전에 '나 = I'나 '고양이 = cat' 처럼 각 단어의 대응 관계를 seq2seq에게 학습시킬 수는 없을까요?\n",
    "앞으로 우리의 목표는 '도착어 단어'와 대응 관계에 있는 '출발어 단어'의 정보를 골라내는 것입니다.  \n",
    "그리고 그 정보를 이용하여 번역을 수행하는 것입니다.  \n",
    "다시 말해, 필요한 정보에만 주목하여 그 정보로부터 시계열 변환을 수행하는 것이 목표입니다.  \n",
    "이 구조를 어텐션이라 부르며, 이번 장의 핵심 주제입니다.  \n",
    "<img src=img/fig8-6.png width='700'>  \n",
    "위 그림처럼 새롭게 '어떤 계산'을 수행하는 계층을 추가할 겁니다.  \n",
    "이 '어떤 계산'이 받는 입력은 두 가지로, 하나는 Encoder로부터 받는 hs이고, 다른 하나는 시각별 LSTM계층의 은닉 상태입니다.  \n",
    "참고로, 지금까지와 똑같이 Encoder의 마지막 은닉 상태 벡터는 Decoder의 첫 번째 LSTM 계층에 전달합니다.  \n",
    "\n",
    "위 신경망으로 하고 싶은 일은 단어들의 얼라인먼트 추출입니다.  \n",
    "각 시각에서 Decoder에 입력된 단어와 대응 관계인 단어의 벡터를 **hs**에서 골라내겠다는 뜻입니다.  \n",
    "예컨대 그림의 Decoder가 \"I\"를 출력할 때, **hs**에서 \"나\"에 대응하는 벡터를 선택하면 됩니다.  \n",
    "그리고 이러한 '선택'작업을 '어떤 계산'으로 해내겠다는 것입니다.  \n",
    "하지만 여기서 문제가 발생합니다. 바로 선택하는 작업은 미분할 수 없다는 점입니다.  \n",
    "\n",
    "이 문제를 해결할 아이디어는 '하나를 선택'하는게 아니라, '모든 것을 선택'한다는 것입니다.  \n",
    "그리고 이때 아래 그림과 같이 각 단어의 중요도를 나타내는 '가중치'를 별도로 계산하도록 합니다.  \n",
    "<img src=img/fig8-7.png width='700'>  \n",
    "<img src=img/fig8-8.png width='700'>  \n",
    "이렇게 맥락 벡터 **c**에는 \"나\" 벡터의 성분이 많이 포함되어 있다는 것입니다.  \n",
    "즉 \"나\" 벡터를 선택하는 작업을 이 가중합으로 대체할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 개선 ②\n",
    "---\n",
    "각 단어의 중요도를 나타내는 가중치 **a**가 있다면, 가중합을 이용해 '맥락 벡터'를 얻을 수 있습니다.  \n",
    "그런데 이 **a**는 어떻게 구해야 할까요?  \n",
    "<img src=img/fig8-12.png width='500'>  \n",
    "그림에서는 Decoder의 LSTM 계층의 은닉 상태 벡터를 **h**라 했습니다.  \n",
    "지금 목표는 이 **h**가 **hs**의 각 단어 벡터와 얼마나 '비슷한가'를 수치로 나타내는 것입니다.  \n",
    "여기에서는 가장 단순한 방법인 벡터의 '내적'을 이용하고자 합니다.  \n",
    "참고로 두 벡터 $\\mathrm{a=(a_{1},a_{2},\\cdots,a_{n})}$와 $\\mathrm{b=(b_{1},b_{2},\\cdots,b_{n})}$의 내적은 다음과 같이 계산합니다.  \n",
    "\n",
    "$$ \\large{\\mathrm{a\\cdot b = a_{1}b_{1} + a_{2}b_{2} + \\cdots + a_{n}b_{n}}} $$  \n",
    "내적의 직관적인 의미는 '두 벡터가 얼마나 같은 방향을 향하고 있는가'입니다.  \n",
    "따라서 두 벡터의 '유사도'를 표현하는 척도로 내적을 이용합니다.  \n",
    "<img src=img/fig8-13.png width='500'>  \n",
    "여기에서는 벡터의 내적을 이용해 **h**와 **hs**의 각 단어 벡터와의 유사도를 구합니다.  \n",
    "그리고 **s**는 그 결과입니다. **s**는 정규화하기 전의 값이며, 소프트맥스 함수를 적용하여 정규화 합니다.  \n",
    "<img src=img/fig8-14.png width='400'>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 개선 ③\n",
    "---\n",
    "지금까지 Decoder 개선안을 두 가지로 나눠 알아보았습니다.  \n",
    "Decoder 개선 ① 에서는 Weight Sum 계층을, Decoder 개선 ② 에서는 Attention weight 계층을 알아보았습니다.  \n",
    "이 두 계층을 하나로 결합하여 Attention 계층으로 부르겠습니다.  \n",
    "<img src=img/fig8-16.png width='700'>  \n",
    "<img src=img/fig8-17.png width='700'>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ar = a.reshape(N, T, 1)#.repeat(T, axis=1)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "\n",
    "        return dhs, da\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh\n",
    "\n",
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh\n",
    "\n",
    "\n",
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 어텐션을 갖춘 seq2seq 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 구현\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from common.seq2seq import Encoder, Seq2seq\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 구현\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq 구현\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 어텐션 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 날짜 형식 변환 문제\n",
    "---\n",
    "이번 절에서는 영어권에서 사용되는 다양한 날짜 형식을 표준 형식으로 변환하는 것이 목표입니다.  \n",
    "<img src=img/fig8-22.png width='500'>  \n",
    "날짜 형식 변환 문제는 간단하지 않고, 입력과 출력 사이에 명확한 대응 관계가 있기 때문입니다.  \n",
    "<img src=img/fig8-23.png width='500'>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어텐션을 갖춘 seq2seq의 학습\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 10[s] | 손실 3.09\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 20[s] | 손실 1.90\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 30[s] | 손실 1.72\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 40[s] | 손실 1.46\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 49[s] | 손실 1.19\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 59[s] | 손실 1.14\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 68[s] | 손실 1.09\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 78[s] | 손실 1.06\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 88[s] | 손실 1.04\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 97[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 107[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 116[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 126[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 135[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 145[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 154[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 164[s] | 손실 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1978-08-11\n",
      "---\n",
      "정확도 0.000%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 10[s] | 손실 1.00\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 20[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 30[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 40[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 52[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 61[s] | 손실 0.99\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 71[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 82[s] | 손실 0.98\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 93[s] | 손실 0.97\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 104[s] | 손실 0.95\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 114[s] | 손실 0.94\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 124[s] | 손실 0.90\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 134[s] | 손실 0.83\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 144[s] | 손실 0.74\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 154[s] | 손실 0.66\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 164[s] | 손실 0.58\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 176[s] | 손실 0.46\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2016-11-08\n",
      "---\n",
      "정확도 51.460%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.35\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 11[s] | 손실 0.30\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 21[s] | 손실 0.21\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 31[s] | 손실 0.14\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 41[s] | 손실 0.09\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 50[s] | 손실 0.07\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 61[s] | 손실 0.05\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 72[s] | 손실 0.04\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 82[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 93[s] | 손실 0.03\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 103[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 113[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 124[s] | 손실 0.02\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 135[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 146[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 156[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 167[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 178[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.900%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 11[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 22[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 32[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 43[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 54[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 65[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 76[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 86[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 97[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 107[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 118[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 129[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 140[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 152[s] | 손실 0.01\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 165[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 176[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 186[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "정확도 99.900%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 10[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 21[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 31[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 41[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 52[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 62[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 72[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 82[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 92[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 102[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 112[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 122[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 132[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 142[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 152[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 163[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 173[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8d0bb4e7fe35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         correct_num += eval_seq2seq(model, question, correct,\n\u001b[1;32m---> 42\u001b[1;33m                                     id_to_char, verbose, is_reverse=True)\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_num\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Deep-Learning-from-scratch-2-Study\\common\\util.py\u001b[0m in \u001b[0;36meval_seq2seq\u001b[1;34m(model, question, correct, id_to_char, verbos, is_reverse)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[0mstart_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m     \u001b[0mguess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;31m# 문자열로 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Deep-Learning-from-scratch-2-Study\\common\\seq2seq.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, xs, start_id, sample_size)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0msampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msampled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f00b5824d988>\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, enc_hs, start_id, sample_size)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mdec_hs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menc_hs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_hs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_hs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Deep-Learning-from-scratch-2-Study\\common\\time_layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, xs)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Deep-Learning-from-scratch-2-Study\\common\\time_layers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, Wx, Wh, b)\u001b[0m\n\u001b[0;32m    105\u001b[0m         '''\n\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mWx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mzeros_like\u001b[1;34m(a, dtype, order, subok)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;31m# needed instead of a 0 to get same result as zeros for for string dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('정확도 %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()\n",
    "\n",
    "# 그래프 그리기\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('에폭')\n",
    "plt.ylabel('정확도')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=img/fig8-25.png width='400'>  \n",
    "<img src=img/fig8-26.png width='500'>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어텐션 시각화\n",
    "---\n",
    "<img src=img/fig8-27.png width='500'>  \n",
    "위 그림은 seq2seq가 시계열 변환을 할 때의 어텐션 가중치를 시각화한 결과입니다.  \n",
    "예컨대 seq2seq가 최초의 \"1\"을 출력할 때는 입력 문장의 \"1\"위치에 표시가 됩니다.  \n",
    "이렇게 세로축(출력)의 \"1983\"과 \"26\"이 가로축(입력)의 \"1983\"과 \"26\"에 대응하고 있습니다.  \n",
    "또한 월을 뜻하는 \"08\"에 입력 문장의 \"AUGUST\"가 대응하고 있는 점은 놀랍습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAQkUlEQVR4nO3dfYylZXnH8e8PdunuwjYgCw0g8tISCt2kQDcWtMUE/AMJCWpNC402trabNgiotdHEJkpMm9gYk9pSzUa0tKGIAUzUpBSsbyVBFHDRXZaqXSouorCglFXkZbn6x3lGxnFenmc5z+y97PeTnOzMmevcc82cPb95zv283KkqJEntOmBvNyBJWpxBLUmNM6glqXEGtSQ1zqCWpMatGGPQJJWkV61HnUh75sQTTxxU/+ijjw6qf/LJJ3vXPvHEE4PG1rx2VtUR830hYwTlAQccUCtW9PsbsHv37t7jPvvss3vakvSCc911141av3379t61mzdvHjS25nVnVW2Y7wtOfUhS4wxqSWqcQS1JjTOoJalxBrUkNc6glqTGLRnUST6a5KEkW5ajIUnSz+uzRf3PwHkj9yFJWsCSQV1VXwKGndIkSZqaqZ1CnmQjsHFa40mSJqYW1FW1CdgEk1PIpzWuJO3vPOpDkhpnUEtS4/ocnnctcBtwcpIdSd40fluSpBlLzlFX1cXL0YgkaX5OfUhS4wxqSWqcQS1JjTOoJalxBrUkNW6UxW2TeGaiXrAOOKD/9s3QBZnXrVvXu/aRRx4ZNPbjjz8+qH7t2rW9a8fIkf2Qi9tK0r7KoJakxhnUktQ4g1qSGmdQS1LjDGpJapxBLUmN6xXUSS5PsiXJ1iRvGbspSdJz+lyPej3wZ8BLgd8ELkhy0tiNSZIm+mxRnwJ8uap+UlXPAF8EXjNuW5KkGX2CegtwdpLDk6wBzgeOnVuUZGOSO5LcMe0mJWl/1meFl21J3gfcAuwC7gaemafuZ6uQe60PSZqeXjsTq+qqqjqjqs4GHgW+NW5bkqQZS25RAyQ5sqoeSvIS4LXAWeO2JUma0SuogRuSHA48DVxSVT8csSdJ0iy9grqqfnfsRiRJ8/PMRElqnEEtSY0zqCWpcQa1JDWu71EfkjpDF6wdYufOnaONfcghhwyqH7JgbZKh7WgAt6glqXEGtSQ1zqCWpMYZ1JLUOINakhpnUEtS4wxqSWqcQS1Jjeu7CvlbuxXItyS5NsmqsRuTJE30WYX8GOAyYENVrQcOBC4auzFJ0kTfqY8VwOokK4A1wPfGa0mSNNuSQV1VDwDvB+4HHgQeq6qb59a5CrkkjaPP1MdhwIXACcDRwMFJXj+3rqo2VdWGqtow/TYlaf/VZ+rjlcB9VfVwVT0N3Ai8bNy2JEkz+gT1/cCZSdZkci3Dc4Ft47YlSZrRZ476duB64C7gG91jNo3clySpkyEXB+89aDL9QSUtKxcOWHZ3LrSPzzMTJalxBrUkNc6glqTGGdSS1DhXIZc0ryE7CIcelODOx2HcopakxhnUktQ4g1qSGmdQS1LjDGpJapxBLUmNM6glqXF9Fg5YleQrSe7uFri9YjkakyRN9Dnh5UngnKralWQlcGuSf6+qL4/cmySJHkFdk1OOdnWfruxuXsZUkpZJrznqJAcm2Qw8BNzSLSYwt8bFbSVpBIMWDkhyKPBJ4NKq2rJInVvc0n7Ea31MxXQWDqiqHwFfAM6bQlOSpB76HPVxRLclTZLVTFYlv3fsxiRJE32O+jgKuDrJgUyC/RNV9Zlx25Ikzehz1MfXgdOXoRdJ0jw8M1GSGmdQS1LjDGpJapxBLUmNM6glqXH71CrkQ89mWr16de/aJ554Ymg7vR1wwLC/h7t37x6pk+FWrVrVu/anP/3piJ2oZevXrx9Uv27dut61p5xyyqCxL7jggt61733veweNvWvXrqWLRuAWtSQ1zqCWpMYZ1JLUOINakhpnUEtS4wxqSWqcQS1JjTOoJalxBrUkNc6glqTGTe0U8iQbgY3TGk+SNDG1oK6qTcAmcBVySZqm3lMfSS5Jsrm7HT1mU5Kk5/Teoq6qK4ErR+xFkjQPdyZKUuMMaklqnEEtSY0zqCWpcQa1JDXOoJakxhnUktS4VE3/JELPTGzX0Od76MrvkvbYnVW1Yb4vuEUtSY0zqCWpcQa1JDXOoJakxhnUktQ4g1qSGmdQS1LjegV1kvOS/HeSbyd559hNSZKes2RQJzmQyYIBrwJOBS5OcurYjUmSJvpsUb8U+HZVba+qp4CPAxeO25YkaUafoD4G+O6sz3d09/2cJBuT3JHkjmk1J0nqt2bifBd7+IULRrgKuSSNo88W9Q7g2Fmfvxj43jjtSJLm6hPUXwVOSnJCkoOAi4BPjduWJGnGklMfVfVMkjcD/wEcCHy0qraO3pkkCfB61Psdr0ctNcvrUUvSvsqglqTGGdSS1DiDWpIa1+eEF72ADN05OGTnozsepXG4RS1JjTOoJalxBrUkNc6glqTGGdSS1DiDWpIaZ1BLUuMMaklqXN9VyN+aZGuSLUmuTbJq7MYkSRN9ViE/BrgM2FBV65lck/qisRuTJE30nfpYAaxOsgJYg0txSdKyWTKoq+oB4P3A/cCDwGNVdfPcOlchl6Rx9Jn6OAy4EDgBOBo4OMnr59ZV1aaq2rDQCgWSpD3TZ+rjlcB9VfVwVT0N3Ai8bNy2JEkz+gT1/cCZSdZkch3Lc4Ft47YlSZrRZ476duB64C7gG91jNo3clySp4yrkWpQLB0jLxlXIJWlfZVBLUuMMaklqnEEtSY3bp1YhX7ly5aD6yy67rHftBz/4wUFjP/30071r165dO2jsXbt29a4dY2fwbOvWretde9ZZZw0ae/v27b1rf/CDHwwa+4QTTuhdu3PnzkFjD3l+hhr7+dwXDd1JPSQnnnrqqaHt7BVuUUtS4wxqSWqcQS1JjTOoJalxBrUkNc6glqTGGdSS1DiDWpIaZ1BLUuMMaklq3NROIU+yEdg4rfEkSRNTC+qq2kS38osLB0jS9PSe+khySZLN3e3oMZuSJD2n9xZ1VV0JXDliL5KkebgzUZIaZ1BLUuMMaklqnEEtSY0zqCWpcQa1JDXOoJakxmWMVY89M1H7kqGvgaGrYks93VlVG+b7glvUktQ4g1qSGmdQS1LjDGpJapxBLUmNM6glqXEGtSQ1bsmgTnJsks8n2ZZka5LLl6MxSdJEn4UDngH+sqruSrIWuDPJLVV1z8i9SZLosUVdVQ9W1V3dx48D24Bjxm5MkjQxaHHbJMcDpwO3z/M1VyGXpBH0vtZHkkOALwJ/U1U3LlHrtT60z/BaH2rE87vWR5KVwA3ANUuFtCRpuvoc9RHgKmBbVX1g/JYkSbP12aJ+OfAG4Jwkm7vb+SP3JUnqLLkzsapuBZyUk6S9xDMTJalxBrUkNc6glqTGGdSS1DiDWpIaN+gUcumFaOiZhkPOZPQsRk2DW9SS1DiDWpIaZ1BLUuMMaklqnEEtSY0zqCWpcQa1JDWu78IBhya5Psm93WrkZ43dmCRpou8JL38P3FRVr0tyELBmxJ4kSbMsGdRJfhk4G3gjQFU9BTw1bluSpBl9pj5OBB4GPpbka0k+kuTguUVJNia5I8kdU+9SkvZjfYJ6BXAG8KGqOh34MfDOuUVVtamqNiy0iq4kac/0CeodwI6qur37/HomwS1JWgZLBnVVfR/4bpKTu7vOBe4ZtStJ0s/0PerjUuCa7oiP7cAfj9eSJGm2XkFdVZsB554laS/wzERJapxBLUmNM6glqXEGtSQ1zqCWpMaNtQr5TuA7c+5b193f15D6McduqRfHXt6x561fZGXxffXndOw2ejluweqqWpYbcMdY9WOO3VIvju1z79j733NfVU59SFLrDGpJatxyBvWmEevHHHtovWO/cMYeWu/YL5yxh9aP2ku6+RJJUqOc+pCkxhnUktS40YM6ye4km2fdju9RuyXJp5Mc2vN77BrQx9Ykdyd5W5JFf/4kr0lSSX59ibokuTXJq2bd9/tJburT/7QN6Pv4JFvm3PeeJG9foP5Xkvxbku1J7kxyW5LXTHH8d3XPz9e75+q3F6g7fNb/p+8neWDW5wct9jP3keTYJJ9Psq3r5/Iejzk0yfVJ7u0ed9bz7WNPJPlokofm/t4Xqb+8e71tTfKWJWrf2tVtSXJtklWL1K5K8pXutbY1yRVDfxbNMuRYvj25Abv2pBa4GnjXtL7HnLGPBD4LXLHEYz4B/Bfwnh7jrwe2AauAg4FvAb869u/3+fQNHA9smXPfe4C3z1Mb4Dbgz2fddxxw6ZTGP6sb/5e6z9cBR/f4Wecd73n+/o4Czug+Xgt8Ezh1icdcDfxp9/FBwKF76bk/m8kKTFt61K4HtgBrmJz89lngpAVqjwHuA1bP+j/2xkXGDnBI9/FK4HbgzL3xO3kh3Fqe+riNyX+Oqauqh4CNwJuzwGlmSQ4BXg68Cbiox5hbgE8D7wDeDfxLVf3P1JruaWjfA5wDPFVVH565o6q+U1X/MKXxjwJ2VtWT3dg7q+p7Uxp7kKp6sKru6j5+nMkf4AX/Lyb5ZSYBeVX3mKeq6kfL0etcVfUl4NGe5acAX66qn1TVM8AXgQXfITEJ89VJVjAJ9wWfn5qYeae7srt55MIeWo6gXj3rbekn+zwgyYFMlvz61FhNVdV2Jj//kQuUvBq4qaq+CTyapM86kVcAfwi8Cvi7qTQ63J703cdvAHdNaaz53Awcm+SbSf4pyStG/F69dVN1pzPZIlzIicDDwMeSfC3JR5IcvAztPV9bgLO7qaQ1wPnAsfMVVtUDwPuB+4EHgceq6ubFBk9yYJLNwEPALfXcuqsaaDmC+omqOq27LfbXGrpQBx4BXgTcMnJvC160AbgY+Hj38ce7zxdVVT8GrgP+dWbLcC8Y0vdCWzhLbvkkubKbf/zqNMbvtr5+i8k7nYeB65K8cak+xtS9O7kBeEtV/d8ipSuYTDd8qKpOB34MvHMZWnxeqmob8D4mr7ObgLuBZ+arTXIYcCFwAnA0cHCS1y8x/u6qOg14MfDSJOun2P5+pbWpjye6J/Y4JvN8l4z1jZKcCOxm8td+7tcOZ/JW/yNJ/hf4K+APFpommePZ7rbs9qDvR4DD5tz3Iua/uMxWZq0+X1WXMHnXc8QiLQ0Zf+aF/YWqejfwZuD3Fhl7VElWMgnpa6rqxiXKdwA7Zm0xXs+s31XLquqqqjqjqs5mMmXyrQVKXwncV1UPV9XTwI3Ay3p+jx8BXwDOm0LL+6XWghqAqnoMuAx4e/eCmaokRwAfBv6xqubb6nsdkznm46rq+Ko6lsmOlN8ZoZf/TDKtufhBfXdbsQ8mObfr5UVMXky3zlP+OWBVkr+Ydd+axZoZMn6Sk5OcNOuu0/jFKzAui+4P21XAtqr6wFL1VfV94LtJTu7uOhe4p8f3meZzv0eSHNn9+xLgtcC1C5TeD5yZZE33+zmXydz9QuMeke6orSSrmQT9vdPsfX/SZFADVNXXmLwVm9YOsZm58q1M9m7fzGROeT4XA3Pn029gMv88NZkcHvhr9N/5s5Q96fuPgL/uppw+x+RImF/YCdr9QXs18Iok9yX5CpMjHd6xRE+9xgcOAa5Ock+SrwOnMjmiY294OfAG4JxZ+1fOX+IxlwLXdL2fBvztYsUjPPcz417LZEf8yUl2JHnTEg+5Ick9THaEX1JVP5yvqHu3cD2T/RTfYJIdi50GfRTw+e738VUmc9SfGfbTaIankO9F3Zzdn1TV2/Z2L1pePvcawqCWpMY1O/UhSZowqCWpcQa1JDXOoJakxhnUktQ4g1qSGvf/UqXi80kRKJkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model.load_params()\n",
    "\n",
    "_idx = 0\n",
    "def visualize(attention_map, row_labels, column_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n",
    "\n",
    "    ax.patch.set_facecolor('black')\n",
    "    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xticklabels(row_labels, minor=False)\n",
    "    ax.set_yticklabels(column_labels, minor=False)\n",
    "\n",
    "    global _idx\n",
    "    _idx += 1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "np.random.seed(1984)\n",
    "for _ in range(5):\n",
    "    idx = [np.random.randint(0, len(x_test))]\n",
    "    x = x_test[idx]\n",
    "    t = t_test[idx]\n",
    "\n",
    "    model.forward(x, t)\n",
    "    d = model.decoder.attention.attention_weights\n",
    "    d = np.array(d)\n",
    "    attention_map = d.reshape(d.shape[0], d.shape[2])\n",
    "\n",
    "    # 출력하기 위해 반전\n",
    "    attention_map = attention_map[:,::-1]\n",
    "    x = x[:,::-1]\n",
    "\n",
    "    row_labels = [id_to_char[i] for i in x[0]]\n",
    "    column_labels = [id_to_char[i] for i in t[0]]\n",
    "    column_labels = column_labels[1:]\n",
    "\n",
    "    visualize(attention_map, row_labels, column_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
