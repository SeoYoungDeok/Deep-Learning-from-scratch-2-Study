{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 [게이트가 추가된 RNN]\n",
    "---\n",
    "앞 장의 단순한 RNN은 시계열 데이터에서 시간적으로 멀리 떨어진, 장기 의존 관계를 잘 학습할 수 없다는 문제가 있습니다.  \n",
    "LSTM, GRU 구조는 '게이트<sup>gate</sup>'라는 구조가 더해져 있는데, 이 게이트 덕분에 시계열 데이터의 장기 의존 관계를 학습할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RNN의 문제점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기울기 소실 또는 기울기 폭발\n",
    "---\n",
    "<img src=img/fig6-5.png width='800'>  \n",
    "RNN 계층에서 시간 방향으로 역전파 할 때 전해지는 기울기는 차례로 'tanh', '+', 'MatMul'연산을 통과합니다.\n",
    "이 계층들이 기울기를 어떻게 변화시킬까요?\n",
    "\n",
    "$ y = tanh(x) $일 때의 미분은 $ \\frac{\\partial y}{\\partial x}=1-y^{2} $ 입니다. 이때 $ y = tanh(x) $의 값과 그 미분 값을 각각 그래프로 그리면 다음과 같습니다.  \n",
    "<img src=img/fig6-6.png width='400'>  \n",
    "점선이 $ y = tanh(x) $의 미분입니다. 보다시피 그 값은 1.0 이하이고, x가 0으로부터 멀어질수록 작아집니다.  \n",
    "즉 역전파에서 기울기가 tanh 노드를 지날 때마다 값은 계속 작아진다는 뜻입니다.  \n",
    "\n",
    "다음은 Matmul 노드 입니다.  \n",
    "상류로부터 $ dh $라는 기울기가 흘러옵니다.  \n",
    "이때 MatMul 노드에서의 역전파는 $ dhW_{n}^{T} $라는 행렬 곱으로 기울기를 계산합니다.  \n",
    "그리고 같은 계산을 시계열 데이터의 시간 크기만큼 반복하죠. 그러면 역전파 시 기울기는 MatMul 노드를 지날 때마다 어떻게 변할까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.4684068094579303, 3.3357049741610365, 4.783279375373182, 6.279587332087612, 8.080776465019053, 10.251163032292936, 12.936063506609896, 16.276861327786712, 20.45482961834598, 25.688972842084684, 32.25315718048336, 40.48895641683869, 50.8244073070191, 63.79612654485427, 80.07737014308985, 100.5129892205125, 126.16331847536823, 158.35920648258823, 198.7710796761195, 249.495615421267]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 49884 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 44036 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 53356 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 44592 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 45432 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 47492 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 49884 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 44036 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 53356 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 44592 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 45432 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 47492 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[165.82723327563536, 46.073751982593116, 39.33717299029768, 15.343673208370237, 13.427630740269848, 7.764749802532891, 5.710756601803507, 3.7195501552126626, 2.5825395665804947, 1.735482186197724, 1.1861180077280082, 0.8036560015488017, 0.5469555307956063, 0.3713955517070608, 0.2524843606451469, 0.17154116983311937, 0.11658374794484584, 0.07922054522299886, 0.0538360964803239, 0.03658396904103571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 49884 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 44036 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 53356 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 44592 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 45432 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:214: RuntimeWarning: Glyph 47492 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 49884 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 44036 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 53356 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 44592 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 45432 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\anaconda\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:183: RuntimeWarning: Glyph 47492 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 2   # 미니배치 크기\n",
    "H = 3   # 은닉 상태 벡터의 차원 수\n",
    "T = 20  # 시계열 데이터의 길이\n",
    "\n",
    "dh = np.ones((N, H))\n",
    "\n",
    "np.random.seed(3) # 재현할 수 있도록 난수의 시드 고정\n",
    "\n",
    "Wh = np.random.randn(H, H)\n",
    "\n",
    "norm_list = []\n",
    "for t in range(T):\n",
    "    dh = np.dot(dh, Wh.T)\n",
    "    norm = np.sqrt(np.sum(dh**2)) / N\n",
    "    norm_list.append(norm)\n",
    "\n",
    "print(norm_list)\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.plot(np.arange(len(norm_list)), norm_list)\n",
    "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20])\n",
    "plt.xlabel('시간 크기(time step)')\n",
    "plt.ylabel('노름(norm)')\n",
    "plt.show()\n",
    "\n",
    "Wh = np.random.randn(H, H) * 0.5\n",
    "\n",
    "norm_list = []\n",
    "for t in range(T):\n",
    "    dh = np.dot(dh, Wh.T)\n",
    "    norm = np.sqrt(np.sum(dh**2)) / N\n",
    "    norm_list.append(norm)\n",
    "\n",
    "print(norm_list)\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.plot(np.arange(len(norm_list)), norm_list)\n",
    "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20])\n",
    "plt.xlabel('시간 크기(time step)')\n",
    "plt.ylabel('노름(norm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프에서 보듯 기울기의 크기는 시간에 비례해 지수적으로 증가(폭발) 혹은 감소(소실)합니다.  \n",
    "이러한 현상 때문에 장기 의존 관계를 학습할 수 없게 됩니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기울기 폭발 대책\n",
    "---\n",
    "기울기 폭발의 대책으로는 **기울기 클리핑**<sup>gradients clipping</sup>이라는 전통적인 기법이 있습니다.  \n",
    "그 알고리즘을 의사 코드로 쓰면 다음과 같습니다.  \n",
    "\n",
    "$$ \\large{\\mathrm{ if\\  \\lVert \\hat{\\mathbf{g}} \\lVert\\ \\ge\\ threshold}}: $$  \n",
    "$$ \\large{\\mathrm{ \\hat{\\mathbf{g}} = \\frac{threshold}{\\lVert \\hat{\\mathbf{g}} \\lVert}\\hat{\\mathbf{g}}}} $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dW1 = np.random.rand(3, 3) * 10\n",
    "dW2 = np.random.rand(3, 3) * 10\n",
    "grads = [dW1, dW2]\n",
    "max_norm = 5.0\n",
    "\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 기울기 소실과 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM의 인터페이스\n",
    "---\n",
    "LSTM 계층을 자세히 살펴보기 전에 계산 그래프를 단순화하는 도법을 사용하겠습니다.  \n",
    "<img src=img/fig6-10.png width='800'>  \n",
    "그림에서 $ tanh(h_{t-1}W_{h} + x_{t}W{x} + b) $ 계산을 tanh라는 직사각형 노드 하나로 그렸습니다.  \n",
    "\n",
    "이제 LSTM과 RNN을 비교하는 것부터 시작해보겠습니다.  \n",
    "<img src=img/fig6-11.png width='800'>  \n",
    "그림처럼 LSTM 계층의 인터페이스에는 $c$라는 경로가 있다는 차이가 있습니다.  \n",
    "이 $c$를 **기억 셀**<sup>memory cell</sup>이라 하며, LSTM 전용의 기억 메커니즘입니다.\n",
    "\n",
    "기억 셀의 특징은 데이터를 자기 자신으로만(LSTM 계층 내에서만) 주고받는다는 것입니다.  \n",
    "즉, LSTM 계층 내에서만 연결되고, 다른 계층으로는 출력하지 않습니다.  \n",
    "LSTM의 은닉상태 h는 RNN 계층과 마찬가지로 다른 계층으로(위쪽으로) 출력됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 계층 조립하기\n",
    "---\n",
    "LSTM의 부품을 하나씩 조립하면서, 그 구조를 차분히 알아보겠습니다.  \n",
    "\n",
    "LSTM에는 기억 셀 $c_{t}$가 있습니다. 이 $c_{t}$에는 시각 $t$에서의 LSTM의 기억이 저장돼 있는데  \n",
    "과거로부터 시각 $t$까지에 필요한 모든 정보가 저장돼 있다고 가정합니다. (혹은 그렇게 되도록 학습을 수행합니다.)  \n",
    "그리고 필요한 정보를 모두 간직한 이 기억을 바탕으로, 외부 계층에(그리고 다음 시각의 LSTM에) 은닉 상태 $h_{t}$를 출력합니다.  \n",
    "<img src=img/fig6-12.png width='500'>  \n",
    "그림처럼 현재의 기억 셀 $c_{t}$는 3개의 입력($c_{t-1}, h_{t-1}, x_{t}$)으로부터 '어떤 계산'을 수행하여 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output 게이트\n",
    "---\n",
    "이 게이트는 $tanh(c_{t})$의 각 원소에 대해 '그것이 다음 시각의 은닉 상태에 얼마나 중요한가'를 조정합니다.  \n",
    "output 게이트의 열림 상태(다음 몇 %만 흘려보낼까)는 입력 $x_{t}$와 이전 상태 $h_{t-1}$로부터 구합니다.  \n",
    "이때의 계산은 다음과 같습니다. 여기서 사용하는 가중치 매개변수와 편향에는 output의 첫 글자인 $o$를 첨자로 추가합니다.  \n",
    "이후에도 마찬가지로 첨자를 붙여 게이트임을 표시하겠습니다. 한편, 시그모이드 함수는 $\\sigma()$로 표기합니다.  \n",
    "\n",
    "$$ \\large{\\mathrm{ o = \\sigma(x_{t}W^{(o)}_{x} + h_{t-1}W^{(o)}_{h} + b^{(o)}) }} $$  \n",
    "<img src=img/fig6-15.png width='500'>  \n",
    "$\\sigma$의 출력을 $o$라고 하면 $h_t$는 $o$와 $tanh(c_t)$의 곱으로 계산됩니다.  \n",
    "여기서 말하는 '곱'이란 원소별 곱이며, 이것을 **아다마르 곱**<sup>Hadamard product</sup>이라고도 합니다.  \n",
    "아다마르 곱은 기호로 $\\odot$으로 나타내며, 다음과 같은 계산을 수행합니다.  \n",
    "\n",
    "$$ \\large{\\mathrm{ h_{t} = o \\odot tanh(c_{t}) }} $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forget 게이트\n",
    "---\n",
    "망각은 더 나은 전진을 낳습니다. 우리가 다음에 해야 할 일은 기억 셀에 '무엇을 잊을까'를 명확하게 지시하는 것입니다.  \n",
    "그러면 $c_{t-1}$의 기억 중에서 불필요한 기억을 잊게 해주는 게이트를 추가하고, 이를 **forget 게이트**(망각 게이트)라 부르도록 하겠습니다.  \n",
    "<img src=img/fig6-16.png width='500'>  \n",
    "$$ \\large{\\mathrm{ f = \\sigma(x_{t}W^{(f)}_{x} + h_{t-1}W^{(f)}_{h} + b^{(f)}) }} $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 새로운 기억 셀\n",
    "---\n",
    "forget 게이트를 거치면서 이전 시각의 기억 셀로부터 잊어야 할 기억이 삭제되었습니다.  \n",
    "그런데 이 상태로는 기억 셀이 잊는 것밖에 하지 못하겠네요. 그래서 새로 기억해야 할 정보를 기억셀에 추가해야 합니다.  \n",
    "<img src=img/fig6-17.png width='500'>  \n",
    "$$ \\large{\\mathrm{ g = tanh(x_{t}W^{(g)}_{x} + h_{t-1}W^{(g)}_{h} + b^{(g)}) }} $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input 게이트\n",
    "---\n",
    "마지막으로 $g$에 게이트를 하나 추가하겠습니다. 여기에서 새롭게 추가하는 게이트를 **input 게이트**(입력 게이트)라고 하겠습니다.  \n",
    "<img src=img/fig6-18.png width='500'>  \n",
    "input 게이트는 $g$의 각 원소가 새로 추가되는 정보로써의 가치가 얼마나 큰지를 판단합니다.  \n",
    "새 정보를 무비판적으로 수용하는게 아니라, 적절히 취사선택하는 것이 이 게이트의 역할입니다.  \n",
    "다른 관점에서 보면, input 게이트에 의해 가중된 정보가 새로 추가되는 셈입니다.\n",
    "\n",
    "$$ \\large{\\mathrm{ i = \\sigma(x_{t}W^{(i)}_{x} + h_{t-1}W^{(i)}_{h} + b^{(i)}) }} $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM의 기울기 흐름\n",
    "---\n",
    "LSTM의 구조를 살표보았는데, 이것이 어떤 원리로 기울기 소실을 없애주는 걸까요?  \n",
    "그 원리는 기억 셀 $c$의 역전파에 주목하면 보입니다.  \n",
    "<img src=img/fig6-19.png width='800'>  \n",
    "기억 셀의 역전파에서는 '+'와 'x'노드만을 지나게 됩니다.  \n",
    "'+'노드는 상류에서 전해지는 기울기를 그대로 흘릴 뿐입니다. 따라서 기울기의 변화(감소)는 일어나지 않습니다.  \n",
    "\n",
    "그럼 'x'노드가 남는데, 이 노드는 '행렬 곱'이 아닌 '원소별 곱(아다마르 곱)'을 계산합니다.  \n",
    "LSTM의 역전파에서는 '행렬 곱'이 아닌 '원소별 곱'이 이뤄지고, 매 시각 다른 게이트 값을 이용해 원소별 곱을 계산합니다.  \n",
    "이처럼 매번 새로운 게이트 값을 이용하므로 곱셈의 효과가 누적되지 않아 기울기 소실이 일어나지 않는 (혹은 일어나기 어려운) 것입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM 구현\n",
    "다음은 LSTM에서 수행하는 계산을 정리한 수식들입니다.  \n",
    "$$ \\large{\\mathrm{ f = \\sigma(x_{t}W^{(f)}_{x} + h_{t-1}W^{(f)}_{h} + b^{(f)}) }} $$\n",
    "$$ \\large{\\mathrm{ g = tanh(x_{t}W^{(g)}_{x} + h_{t-1}W^{(g)}_{h} + b^{(g)}) }} $$\n",
    "$$ \\large{\\mathrm{ i = \\sigma(x_{t}W^{(i)}_{x} + h_{t-1}W^{(i)}_{h} + b^{(i)}) }} $$\n",
    "$$ \\large{\\mathrm{ o = \\sigma(x_{t}W^{(o)}_{x} + h_{t-1}W^{(o)}_{h} + b^{(o)}) }} $$\n",
    "\n",
    "$$ \\large{\\mathrm{ c_{t} = f \\odot c_{t-1} + g \\odot i }} $$\n",
    "$$ \\large{\\mathrm{ h_{t} = o \\odot tanh(c_{t}) }} $$  \n",
    "\n",
    "$\\mathrm{f, g, i, o}$는 하나의 식으로 정리해 계산할 수 있습니다.  \n",
    "<img src=img/fig6-20.png width='700'>  \n",
    "4개의 가중치를 하나로 모을 수 있고, 그렇게 하면 원래 개별적으로 총 4번을 수행하던 아핀 변환을 단 1회의 계산으로 끝마칠 수 있습니다.  \n",
    "<img src=img/fig6-21.png width='700'>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        '''\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: 입력 x에 대한 가중치 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        Wh: 은닉 상태 h에 대한 가장추 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        b: 편향（4개분의 편향이 담겨 있음）\n",
    "        '''\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape\n",
    "\n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
    "\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "\n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "\n",
    "        c_next = f * c_prev + g * i\n",
    "        h_next = o * np.tanh(c_next)\n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "\n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "\n",
    "        dc_prev = ds * f\n",
    "\n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "\n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= (1 - g ** 2)\n",
    "\n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "\n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        db = dA.sum(axis=0)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "\n",
    "        return dx, dh_prev, dc_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time LSTM 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLSTM:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = Wh.shape[0]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params)\n",
    "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
    "            hs[:, t, :] = self.h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
    "            dxs[:, t, :] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h, c=None):\n",
    "        self.h, self.c = h, c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h, self.c = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LSTM을 사용한 언어 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "언어 모델은 앞 챕터에서 구현한 모델에서 RNN계층이 LSTM계층으로 바뀐것이 유일한 차이점 입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from common.base_model import BaseModel\n",
    "\n",
    "\n",
    "class Rnnlm(BaseModel):\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 1327 | 시간 0[s] | 퍼플렉서티 10002.16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-af08074c21f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# 기울기 클리핑을 적용하여 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\n\u001b[0m\u001b[0;32m     32\u001b[0m             eval_interval=20)\n\u001b[0;32m     33\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Deep-Learning-from-scratch-2-Study-master\\common\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, xs, ts, max_epoch, batch_size, time_size, max_grad, eval_interval)\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[1;31m# 기울기를 구해 매개변수 갱신\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 공유된 가중치를 하나로 모음\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmax_grad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-f506222b20e9>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Deep-Learning-from-scratch-2-Study-master\\common\\time_layers.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m         \u001b[0mdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.util import eval_perplexity\n",
    "from dataset import ptb\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 20\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # RNN의 은닉 상태 벡터의 원소 수\n",
    "time_size = 35     # RNN을 펼치는 크기\n",
    "lr = 20.0\n",
    "max_epoch = 4\n",
    "max_grad = 0.25\n",
    "\n",
    "# 학습 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "\n",
    "# 모델 생성\n",
    "model = Rnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "# 기울기 클리핑을 적용하여 학습\n",
    "trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\n",
    "            eval_interval=20)\n",
    "trainer.plot(ylim=(0, 500))\n",
    "\n",
    "# 테스트 데이터로 평가\n",
    "model.reset_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)\n",
    "print('테스트 퍼플렉서티: ', ppl_test)\n",
    "\n",
    "# 매개변수 저장\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RNNLM 추가 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 계층 다층화\n",
    "---\n",
    "<img src=img/fig6-29.png width='700'>  \n",
    "지금까지 우리는 LSTM 계층을 1층만 사용했지만 이를 2층, 3층으로 여러 겹 쌓으면 언어 모델의 정확도가 향상되리라 기대할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 드롭아웃에 의한 과적합 억제\n",
    "---\n",
    "층을 깊게 쌓음으로써 표현력이 풍부한 모델을 만들 수 있죠. 그러나 이런 모델은 종종 **과적합**<sup>overfitting</sup>을 일으킵니다.  \n",
    "과적합을 억제하는 전통적인 방법이 있습니다. '훈련 데이터의 양 늘리기'와 '모델의 복잡도 줄이기'가 있습니다.  \n",
    "그 외에는 모델의 복잡도에 페널티를 주는 **정규화**도 효과적입니다.  \n",
    "또, 드롭아웃 처럼 훈련 시 계층 내의 뉴런 몇개를 무작위로 무시하고 학습하는 방법도 일종의 정규화라고 할 수 있습니다.  \n",
    "<img src=img/fig6-30.png width='600'>  \n",
    "피드포워드 신경망에서는 드롭아웃 계층을 신경망의 진행방향에 삽입했습니다.  \n",
    "그러나 LSTM은 진행방향(시계열 방향)으로 삽입하면 학습 시 시간의 흐름에 따라 정보가 사라질 수 있습니다.\n",
    "따라서 드롭아웃 계층을 깊이 방향(상하 방향)으로 삽입하는 것이 좋습니다.\n",
    "<img src=img/fig6-32.png width='600'>  \n",
    "<img src=img/fig6-33.png width='600'>  \n",
    "이렇게 구성하면 시간 방향으로 아무리 진행해도 정보를 잃지 않습니다.  \n",
    "\n",
    "그런데 최근 연구에서는 RNN의 시간 방향 정규화를 목표로 하는 방법이 다양하게 제안되고 있습니다.  \n",
    "이 **변형 드롭아웃**<sup>Variational Dropout</sup>을 제안했고, 시간방향으로 적용하는 데 성공했습니다.  \n",
    "<img src=img/fig6-34.png width='600'>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 공유\n",
    "---\n",
    "언어 모델을 개선하는 아주 간단한 트릭 중 **가중치 공유**<sup>weight tying</sup>가 있습니다.  \n",
    "그림에서 보듯 Embedding 계층과 Affine계층이 가중치를 공유합니다.  \n",
    "그러면 학습하는 매개변수 수가 크게 줄어드는 동시에 정확도도 향상되는 일석이조의 기술입니다.  \n",
    "매개변수 수가 줄어든다는 것은 학습하기가 쉬워지고, 과적합이 억제되는 혜택으로 이어집니다.  \n",
    "<img src=img/fig6-35.png width='600'>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개선된 RNNLM 구현\n",
    "---\n",
    "지금까지의 개선점 3가지를 사용하여 BetterRnnlm 클래스로 만들어 보겠습니다.  \n",
    "<img src=img/fig6-36.png width='400'>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "from common.np import *  # import numpy as np\n",
    "from common.base_model import BaseModel\n",
    "\n",
    "\n",
    "class BetterRnnlm(BaseModel):\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=650,\n",
    "                 hidden_size=650, dropout_ratio=0.5):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b1 = np.zeros(4 * H).astype('f')\n",
    "        lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b2 = np.zeros(4 * H).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeAffine(embed_W.T, affine_b)  # weight tying!!\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layers = [self.layers[2], self.layers[4]]\n",
    "        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs, train_flg=False):\n",
    "        for layer in self.drop_layers:\n",
    "            layer.train_flg = train_flg\n",
    "\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts, train_flg=True):\n",
    "        score = self.predict(xs, train_flg)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        for layer in self.lstm_layers:\n",
    "            layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ptb.valid.txt ... \n",
      "Done\n",
      "| 에폭 1 |  반복 1 / 1327 | 시간 7[s] | 퍼플렉서티 9999.99\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4684620a290c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mbest_ppl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'inf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,\n\u001b[0m\u001b[0;32m     45\u001b[0m                 time_size=time_size, max_grad=max_grad)\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Deep-Learning-from-scratch-2-Study-master\\common\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, xs, ts, max_epoch, batch_size, time_size, max_grad, eval_interval)\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[1;31m# 기울기를 구해 매개변수 갱신\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 공유된 가중치를 하나로 모음\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmax_grad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-6e29f31acad8>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Deep-Learning-from-scratch-2-Study-master\\common\\time_layers.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0mdx\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mdx\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[0mdx\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# ignore_labelㅇㅔ 해당하는 데이터는 기울기를 0으로 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[0mdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common import config\n",
    "# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).\n",
    "# ==============================================\n",
    "# config.GPU = True\n",
    "# ==============================================\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.util import eval_perplexity, to_gpu\n",
    "from dataset import ptb\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 20\n",
    "wordvec_size = 650\n",
    "hidden_size = 650\n",
    "time_size = 35\n",
    "lr = 20.0\n",
    "max_epoch = 40\n",
    "max_grad = 0.25\n",
    "dropout = 0.5\n",
    "\n",
    "# 학습 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_val, _, _ = ptb.load_data('val')\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "\n",
    "if config.GPU:\n",
    "    corpus = to_gpu(corpus)\n",
    "    corpus_val = to_gpu(corpus_val)\n",
    "    corpus_test = to_gpu(corpus_test)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "\n",
    "model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "best_ppl = float('inf')\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,\n",
    "                time_size=time_size, max_grad=max_grad)\n",
    "\n",
    "    model.reset_state()\n",
    "    ppl = eval_perplexity(model, corpus_val)\n",
    "    print('검증 퍼플렉서티: ', ppl)\n",
    "\n",
    "    if best_ppl > ppl:\n",
    "        best_ppl = ppl\n",
    "        model.save_params()\n",
    "    else:\n",
    "        lr /= 4.0\n",
    "        optimizer.lr = lr\n",
    "\n",
    "    model.reset_state()\n",
    "    print('-' * 50)\n",
    "\n",
    "\n",
    "# 테스트 데이터로 평가\n",
    "model.reset_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)\n",
    "print('테스트 퍼플렉서티: ', ppl_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
